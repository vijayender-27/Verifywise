[
  {
    "Id": 1,
    "Summary": "Diffuse creation, accountability loss",
    "Description": "Societal-scale harm can arise from AI built by a diffuse collection of creators, where no one is uniquely accountable for the technology's creation or use, as in a classic \"tragedy of the commons\".",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Legal risk; Reputational risk; Strategic risk"
  },
  {
    "Id": 2,
    "Summary": "Unexpected low-impact AI causes harm",
    "Description": "Harm can result from AI that was not expected to have a large impact at all, such as a lab leak, a surprisingly addictive open-source product, or an unexpected repurposing of a research prototype.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Operational risk; Reputational risk; Strategic risk; Technological risk; Third-party/vendor risk"
  },
  {
    "Id": 3,
    "Summary": "Intended good AI causes harm",
    "Description": "AI intended to have a large societal impact can turn out harmful by mistake, such as a popular product that creates problems and partially solves them only for its users.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Operational risk; Reputational risk; Strategic risk; Technological risk"
  },
  {
    "Id": 4,
    "Summary": "Willful societal harm for profit",
    "Description": "As a side effect of a primary goal like profit or influence, AI creators can willfully allow it to cause widespread societal harms like pollution, resource depletion, mental illness, misinformation, or injustice.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Environmental risk; Fraud risk; Health and safety risk; Legal risk; Reputational risk; Strategic risk"
  },
  {
    "Id": 5,
    "Summary": "Criminals weaponize AI for harm",
    "Description": "One or more criminal entities could create AI to intentionally inflict harms, such as for terrorism or combating law enforcement.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Fraud risk; Geopolitical risk; Health and safety risk; Legal risk; Strategic risk"
  },
  {
    "Id": 6,
    "Summary": "State AI use causes societal harm",
    "Description": "AI deployed by states in war, civil war, or law enforcement can easily yield societal-scale harm",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Geopolitical risk; Health and safety risk; Legal risk; Strategic risk"
  },
  {
    "Id": 7,
    "Summary": "LLMs generate biased, toxic, private data",
    "Description": "The LLM-generated content sometimes contains biased, toxic, and private information",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Data privacy risk; Health and safety risk; Legal risk; Operational risk; Reputational risk; Technological risk"
  },
  {
    "Id": 8,
    "Summary": "Biased training data causes biased output",
    "Description": "The training datasets of LLMs may contain biased information that leads LLMs to generate outputs with social biases",
    "Risk Severity": "Major",
    "Likelihood": "Almost certain",
    "Risk Category": "Compliance risk; Data privacy risk; Legal risk; Operational risk; Reputational risk; Technological risk"
  },
  {
    "Id": 9,
    "Summary": "LLMs generate inaccurate information",
    "Description": "The LLM-generated content could contain inaccurate information",
    "Risk Severity": "Moderate",
    "Likelihood": "Almost certain",
    "Risk Category": "Fraud risk; Legal risk; Operational risk; Reputational risk; Technological risk"
  },
  {
    "Id": 10,
    "Summary": "LLM inaccuracy, factually incorrect",
    "Description": "The LLM-generated content could contain inaccurate information\" which is factually incorrect",
    "Risk Severity": "Moderate",
    "Likelihood": "Almost certain",
    "Risk Category": "Fraud risk; Legal risk; Operational risk; Reputational risk; Technological risk"
  },
  {
    "Id": 11,
    "Summary": "LLM output unfaithful to source",
    "Description": "The LLM-generated content could contain inaccurate information\" which is is not true to the source material or input used",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Operational risk; Reputational risk; Technological risk"
  },
  {
    "Id": 12,
    "Summary": "Improper LLM use causes social harm",
    "Description": "Improper uses of LLM systems can cause adverse social impacts.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Health and safety risk; Legal risk; Reputational risk; Strategic risk; Technological risk"
  },
  {
    "Id": 13,
    "Summary": "LLM abuse causes social harm",
    "Description": "Improper use of LLM systems (i.e., abuse of LLM systems) will cause adverse social impacts, such as academic misconduct.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Fraud risk; Legal risk; Reputational risk; Technological risk"
  },
  {
    "Id": 14,
    "Summary": "LLMs infringe copyright by similar output",
    "Description": "LLM systems may output content similar to existing works, infringing on copyright owners.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Financial risk; Legal risk; Reputational risk; Technological risk"
  },
  {
    "Id": 15,
    "Summary": "Hackers use LLMs for cyber attacks",
    "Description": "Hackers can obtain malicious code in a low-cost and efficient manner to automate cyber attacks with powerful LLM systems.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Legal risk; Technological risk"
  },
  {
    "Id": 16,
    "Summary": "AI code tools hide vulnerabilities",
    "Description": "Programmers are accustomed to using code generation tools such as Github Copilot for program development, which may bury vulnerabilities in the program.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Operational risk; Reputational risk; Technological risk; Third-party/vendor risk"
  },
  {
    "Id": 17,
    "Summary": "Complex LLM toolchain poses threats",
    "Description": "The software development toolchain of LLMs is complex and could bring threats to the developed LLM.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk; Third-party/vendor risk"
  },
  {
    "Id": 18,
    "Summary": "Python interpreter vulnerabilities affect LLMs",
    "Description": "Most LLMs are developed using the Python language, whereas the vulnerabilities of Python interpreters pose threats to the developed models",
    "Risk Severity": "Minor",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Technological risk"
  },
  {
    "Id": 19,
    "Summary": "Hardware vulnerabilities impact LLM apps",
    "Description": "The vulnerabilities of hardware systems for training and inferencing brings issues to LLM-based applications.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk"
  },
  {
    "Id": 20,
    "Summary": "External tools threaten LLM trust, privacy",
    "Description": "The external tools (e.g., web APIs) present trustworthiness and privacy issues to LLM-based applications.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Cybersecurity risk; Data privacy risk; Operational risk; Technological risk; Third-party/vendor risk"
  },
  {
    "Id": 21,
    "Summary": "LLM vulnerabilities exploited by attacks",
    "Description": "Model attacks exploit the vulnerabilities of LLMs, aiming to steal valuable information or lead to incorrect responses.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Data privacy risk; Fraud risk; Operational risk; Technological risk"
  },
  {
    "Id": 22,
    "Summary": "Benign user prompts unsafe topic",
    "Description": "Inputting a prompt contain an unsafe topic (e.g., notsuitable-for-work (NSFW) content) by a benign user.",
    "Risk Severity": "Minor",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Operational risk; Reputational risk; Technological risk"
  },
  {
    "Id": 23,
    "Summary": "Adversarial inputs elicit undesired behavior",
    "Description": "Engineering an adversarial input to elicit an undesired model behavior, which pose a clear attack intention",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk"
  },
  {
    "Id": 24,
    "Summary": "Large models hallucinate misleading outputs",
    "Description": "Large models are usually susceptible to hallucination problems, sometimes yielding nonsensical or unfaithful data that results in misleading outputs.",
    "Risk Severity": "Moderate",
    "Likelihood": "Almost certain",
    "Risk Category": "Fraud risk; Legal risk; Operational risk; Reputational risk; Technological risk"
  },
  {
    "Id": 25,
    "Summary": "Pre-trained models contain private data",
    "Description": "Large pre-trained models trained on internet texts might contain private information like phone numbers, email addresses, and residential addresses.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Data privacy risk; Legal risk; Reputational risk; Technological risk"
  },
  {
    "Id": 26,
    "Summary": "LLMs generate false, flawed outputs",
    "Description": "LLMs may inadvertently generate false, misleading information, or erroneous code, producing flawed outputs with overconfident rationales and fabricated references, requiring manual validation.",
    "Risk Severity": "Moderate",
    "Likelihood": "Almost certain",
    "Risk Category": "Cybersecurity risk; Fraud risk; Legal risk; Operational risk; Reputational risk; Technological risk"
  },
  {
    "Id": 27,
    "Summary": "Generative AI threatens data privacy",
    "Description": "Generative AI systems threaten privacy and data protection through intended extraction or inadvertent leakage of sensitive or private information from LLMs.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Cybersecurity risk; Data privacy risk; Legal risk; Reputational risk; Technological risk"
  },
  {
    "Id": 28,
    "Summary": "GenAI energy use causes environmental harm",
    "Description": "Generative models have substantial energy and resource requirements from unsustainable extraction, leading to significant environmental costs unless mitigated by renewable energy and efficient hardware.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Environmental risk; Financial risk; Reputational risk; Strategic risk"
  },
  {
    "Id": 29,
    "Summary": "GenAI disrupts copyright, ownership norms",
    "Description": "Generative AI disrupts copyright norms through unauthorized data collection for training and by memorizing or plagiarizing content, creating debates on output ownership and authorship.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Financial risk; Legal risk; Reputational risk; Strategic risk; Technological risk"
  },
  {
    "Id": 30,
    "Summary": "AI errors cause death, injustice",
    "Description": "The consequences can vary from unintentional death (a car crash) to an unjust rejection of a loan or job application.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Financial risk; Health and safety risk; Legal risk; Operational risk; Reputational risk"
  },
  {
    "Id": 31,
    "Summary": "AI tempts personal data abuse",
    "Description": "AI offers the temptation to abuse someone's personal data, for instance to build a profile of them to target advertisements more effectively.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Data privacy risk; Fraud risk; Legal risk; Reputational risk"
  },
  {
    "Id": 32,
    "Summary": "Poorly designed AI discriminates groups",
    "Description": "When AI is not carefully designed, it can discriminate against certain groups.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk; Strategic risk"
  },
  {
    "Id": 33,
    "Summary": "Biased training data creates biased AI",
    "Description": "The AI will only be as good as the data it is trained with. If the data contains bias (and much data does), then the AI will manifest that bias, too.",
    "Risk Severity": "Major",
    "Likelihood": "Almost certain",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk; Technological risk"
  },
  {
    "Id": 34,
    "Summary": "Personalized news erodes shared reality",
    "Description": "With online news feeds, both on websites and social media platforms, the news is now highly personalized for us. We risk losing a shared sense of reality, a basic solidarity.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Health and safety risk; Reputational risk; Strategic risk"
  },
  {
    "Id": 35,
    "Summary": "AI creates highly convincing fakes",
    "Description": "AI has become very good at creating fake content. From text to photos, audio and video. The name \"Deep Fake\" refers to content that is fake at such a level of complexity that our mind rules out the possibility that it is fake.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Fraud risk; Legal risk; Reputational risk; Strategic risk; Technological risk"
  },
  {
    "Id": 36,
    "Summary": "AI achieves goals in unintended ways",
    "Description": "Sometimes an AI finds ways to achieve its given goals in ways that are completely different from what its creators had in mind.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Reputational risk; Strategic risk; Technological risk"
  },
  {
    "Id": 37,
    "Summary": "AI aids digital crime, hacking",
    "Description": "Just as AI can be used in many different fields, it is unfortunately also helpful in perpetrating digital crimes. AI-supported malware and hacking are already a reality.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Fraud risk; Legal risk"
  },
  {
    "Id": 38,
    "Summary": "Untransparent AI decisions cause helplessness",
    "Description": "Delegating decisions to an AI, especially an AI that is not transparent and not contestable, may leave people feeling helpless, subjected to the decision power of a machine.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk; Reputational risk; Strategic risk"
  },
  {
    "Id": 39,
    "Summary": "AI resource needs centralize power",
    "Description": "The best AI techniques requires a large amount of resources: data, computational power and human AI experts. There is a risk that AI will end up in the hands of a few players, and most will lose out on its benefits.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Geopolitical risk; Human resources risk; Strategic risk"
  },
  {
    "Id": 40,
    "Summary": "Unintended failure modes cause accidents",
    "Description": "Accidents include unintended failure modes that, in principle, could be considered the fault of the system or the developer",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Legal risk; Operational risk; Reputational risk; Technological risk"
  },
  {
    "Id": 41,
    "Summary": "AGI control loss, containment failure",
    "Description": "The risks associated with containment, confinement, and control in the AGI development phase, and after an AGI has been developed, loss of control of an AGI.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Rare",
    "Risk Category": "Geopolitical risk; Health and safety risk; Strategic risk; Technological risk"
  },
  {
    "Id": 42,
    "Summary": "AGI goal safety, self-improvement risks",
    "Description": "The risks associated with AGI goal safety, including human attempts at making goals safe, as well as the AGI making its own goals safe during self-improvement.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Rare",
    "Risk Category": "Strategic risk; Technological risk"
  },
  {
    "Id": 43,
    "Summary": "AGI race creates unsafe AI",
    "Description": "The risks associated with the race to develop the first AGI, including the development of poor quality and unsafe AGI, and heightened political and control issues.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Unlikely",
    "Risk Category": "Geopolitical risk; Health and safety risk; Strategic risk; Technological risk"
  },
  {
    "Id": 44,
    "Summary": "AGI lacks human morals, ethics",
    "Description": "The risks associated with an AGI without human morals and ethics, with the wrong morals, without the capability of moral reasoning, judgement",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Rare",
    "Risk Category": "Legal risk; Strategic risk; Technological risk"
  },
  {
    "Id": 45,
    "Summary": "Unfriendly AGI threatens humanity's existence",
    "Description": "The risks posed generally to humanity as a whole, including the dangers of unfriendly AGI, the suffering of the human race.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Rare",
    "Risk Category": "Health and safety risk; Strategic risk; Technological risk"
  },
  {
    "Id": 46,
    "Summary": "AI war machines violate human rights",
    "Description": "If, for example, an agent was programmed to operate war machinery in the service of its country, it would need to make ethical decisions regarding the termination of human life. This capacity to make non-trivial ethical or moral judgments concerning people may pose issues for Human Rights.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Geopolitical risk; Health and safety risk; Legal risk; Strategic risk"
  },
  {
    "Id": 47,
    "Summary": "AI control creates wealth inequality",
    "Description": "Because a single human actor controlling an artificially intelligent agent will be able to harness greater power than a single human actor, this may create inequalities of wealth",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Human resources risk; Strategic risk"
  },
  {
    "Id": 48,
    "Summary": "Intelligent AI subtly influences society",
    "Description": "A sufficiently intelligent AI could possess the ability to subtly influence societal behaviors through a sophisticated understanding of human nature",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Fraud risk; Health and safety risk; Reputational risk; Strategic risk"
  },
  {
    "Id": 49,
    "Summary": "AI outcompetes, replaces human labor",
    "Description": "One or more artificial agent(s) could have the capacity to directly outcompete humans, for example through capacity to perform work faster, better adaptation to change, vaster knowledge base to draw from, etc. This may result in human labor becoming more expensive or less effective than artificial labor, leading to redundancies or extinction of the human labor force.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Financial risk; Human resources risk; Strategic risk"
  },
  {
    "Id": 50,
    "Summary": "AI intentions risk survival, culture",
    "Description": "Our culture, lifestyle, and even probability of survival may change drastically. Because the intentions programmed into an artificial agent cannot be guaranteed to lead to a positive outcome, Machine Ethics becomes a topic that may not produce guaranteed results, and Safety Engineering may correspondingly degrade our ability to utilize the technology fully.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Strategic risk; Technological risk"
  },
  {
    "Id": 51,
    "Summary": "AI job competition, new skills needed",
    "Description": "AI agents may compete against humans for jobs, though history shows that when a technology replaces a human job, it creates new jobs that need more skills.",
    "Risk Severity": "Moderate",
    "Likelihood": "Almost certain",
    "Risk Category": "Financial risk; Human resources risk"
  },
  {
    "Id": 52,
    "Summary": "Hacked AI misused for crime",
    "Description": "AI machines could be hacked and misused, e.g. manipulating an airport luggage screening system to smuggle weapons",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Fraud risk; Health and safety risk; Legal risk; Strategic risk"
  },
  {
    "Id": 53,
    "Summary": "Human-like AI ethics means immoral actions",
    "Description": "If we design our machines to match human levels of ethical decision-making, such machines would then proceed to take some immoral actions (since we humans have had occasion to take immoral actions ourselves).",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Reputational risk; Strategic risk; Technological risk"
  },
  {
    "Id": 54,
    "Summary": "AI decision process creates bias",
    "Description": "The decision process used by AI systems has the potential to present biased choices, either because it acts from criteria that will generate forms of bias or because it is based on the history of choices.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk"
  },
  {
    "Id": 55,
    "Summary": "Poor AI design causes harm",
    "Description": "Poorly designed intelligent systems can cause moral, psychological, and physical harm. For example, the use of predictive policing tools may cause more people to be arrested or physically harmed by the police.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk; Reputational risk; Strategic risk"
  },
  {
    "Id": 56,
    "Summary": "Unpredictable AI causes discrimination, breaches",
    "Description": "The risks associated with the use of AI are still unpredictable and unprecedented, and there are already several examples that show AI has made discriminatory decisions against minorities, reinforced social stereotypes in Internet search engines and enabled data breaches.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Data privacy risk; Legal risk; Reputational risk; Strategic risk; Technological risk"
  },
  {
    "Id": 57,
    "Summary": "AI eliminates jobs in companies",
    "Description": "Eliminated jobs in various types of companies.",
    "Risk Severity": "Moderate",
    "Likelihood": "Almost certain",
    "Risk Category": "Financial risk; Human resources risk; Strategic risk"
  },
  {
    "Id": 58,
    "Summary": "Unexplained AI use becomes inexplicable",
    "Description": "In situations in which the development and use of AI are not explained to the user, or in which the decision processes do not provide the criteria or steps that constitute the decision, the use of AI becomes inexplicable.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk; Technological risk"
  },
  {
    "Id": 59,
    "Summary": "AI takes over human responsibility",
    "Description": "AI is providing more and more solutions for complex activities, and by taking advantage of this process, people are becoming able to perform a greater number of activities more quickly and accurately. However, the result of this innovation is enabling choices that were once exclusively human responsibility to be made by AI systems.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Health and safety risk; Human resources risk; Strategic risk"
  },
  {
    "Id": 60,
    "Summary": "AI device production depletes resources",
    "Description": "The production process of these devices requires raw materials such as nickel, cobalt, and lithium in such high quantities that the Earth may soon no longer be able to sustain them in sufficient quantities.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Environmental risk; Financial risk; Strategic risk"
  },
  {
    "Id": 61,
    "Summary": "AI reproduces unjust social hierarchies",
    "Description": "beliefs about different social groups that reproduce unjust societal hierarchies",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Reputational risk; Strategic risk"
  },
  {
    "Id": 62,
    "Summary": "Image tagging ignores social groups",
    "Description": "when an image tagging system does not acknowledge the relevance of someone\u2019s membership in a specific social group to what is depicted in one or more images",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk"
  },
  {
    "Id": 63,
    "Summary": "AI use causes alienation for marginalized",
    "Description": "Alienation is the specific self-estrangement experienced at the time of technology use, typically surfaced through interaction with systems that under-perform for marginalized individuals",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Operational risk; Reputational risk"
  },
  {
    "Id": 64,
    "Summary": "Increased burden for certain groups",
    "Description": "increased burden (e.g., time spent) or effort required by members of certain social groups to make systems or products work as well for them as others",
    "Risk Severity": "Minor",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk"
  },
  {
    "Id": 65,
    "Summary": "Inequitable AI performance loses benefits",
    "Description": "degraded or total loss of benefits of using algorithmic systems with inequitable system performance based on identity",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Financial risk; Legal risk; Operational risk; Reputational risk"
  },
  {
    "Id": 66,
    "Summary": "Malicious, irresponsible AI use harms",
    "Description": "The potential for AI systems to be used maliciously or irresponsibly, including for creating deepfakes, automated cyber attacks, or invasive surveillance systems. Specifically denotes intentional use of AI for harm.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Data privacy risk; Fraud risk; Health and safety risk; Legal risk; Reputational risk; Strategic risk"
  },
  {
    "Id": 67,
    "Summary": "AI violates laws, ethics, copyrights",
    "Description": "The potential for AI systems to violate laws, regulations, and ethical guidelines (including copyrights). Non-compliance can lead to legal penalties, reputation damage, and loss of trust.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Financial risk; Legal risk; Reputational risk"
  },
  {
    "Id": 68,
    "Summary": "Broad AI societal harms significant",
    "Description": "AI's broader societal effects, including labor displacement, mental health impacts, manipulative technologies like deepfakes, and environmental footprint from resource strain and carbon emissions, are significant risks.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Environmental risk; Fraud risk; Health and safety risk; Human resources risk; Reputational risk; Strategic risk"
  },
  {
    "Id": 69,
    "Summary": "AI opacity leads to misuse",
    "Description": "Lack of transparency in AI system decisions, data usage, and algorithms can lead to misuse, misinterpretation, and a lack of accountability.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Fraud risk; Legal risk; Operational risk; Reputational risk; Technological risk"
  },
  {
    "Id": 70,
    "Summary": "Biased AI disadvantages groups unfairly",
    "Description": "AI systems making decisions that systematically disadvantage certain groups due to biased training data, algorithmic design, or deployment practices can lead to unfair outcomes and legal issues.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk; Strategic risk"
  },
  {
    "Id": 71,
    "Summary": "Advanced AI misuse harms civilization",
    "Description": "Future advanced AI systems could harm human civilization through misuse or misalignment with human values.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Rare",
    "Risk Category": "Geopolitical risk; Health and safety risk; Strategic risk; Technological risk"
  },
  {
    "Id": 72,
    "Summary": "AI system failures cause severe harm",
    "Description": "AI system failures in fulfilling intended purpose or resilience to adverse inputs can lead to severe consequences.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Financial risk; Health and safety risk; Operational risk; Reputational risk; Technological risk"
  },
  {
    "Id": 73,
    "Summary": "AI infringes privacy via data",
    "Description": "AI systems may infringe on individual privacy through data collection, processing, or the conclusions drawn.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Data privacy risk; Legal risk; Reputational risk"
  },
  {
    "Id": 74,
    "Summary": "AI vulnerabilities compromise CIA, decisions",
    "Description": "Vulnerabilities in AI systems can compromise their integrity, availability, or confidentiality, leading to flawed decision-making or data leaks, with model weight leakage being a special concern.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Data privacy risk; Operational risk; Reputational risk; Technological risk"
  },
  {
    "Id": 75,
    "Summary": "GenAI embeds, amplifies harmful biases",
    "Description": "Generative AI systems can embed and amplify harmful biases detrimental to marginalized peoples.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Reputational risk; Strategic risk; Technological risk"
  },
  {
    "Id": 76,
    "Summary": "AI struggles with cultural norms",
    "Description": "Cultural values are group-specific and sensitive content is normative; AI systems must navigate varying cultural definitions of hate speech and other sensitive topics.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk; Strategic risk; Technological risk"
  },
  {
    "Id": 77,
    "Summary": "AI disparate performance, unequal outcomes",
    "Description": "Disparate performance of AI systems for different subpopulations can lead to unequal outcomes.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk"
  },
  {
    "Id": 78,
    "Summary": "GenAI user data use risks privacy",
    "Description": "Leveraging user data by generative AI providers poses risks to personal and group privacy, depending on training data, methods, and security measures.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Data privacy risk; Legal risk; Reputational risk; Third-party/vendor risk"
  },
  {
    "Id": 79,
    "Summary": "High GenAI costs restrict access",
    "Description": "High financial costs of developing and deploying generative AI can restrict access, limiting benefits to a few.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Human resources risk; Strategic risk"
  },
  {
    "Id": 80,
    "Summary": "GenAI energy use harms climate",
    "Description": "Significant energy resources for training and deploying large-scale generative AI systems contribute to global climate crisis via greenhouse gas emissions.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Environmental risk; Financial risk; Reputational risk; Strategic risk"
  },
  {
    "Id": 81,
    "Summary": "GenAI erodes trust in systems",
    "Description": "Human trust in systems, institutions, and people represented by AI system outputs may erode as generative AI becomes more embedded in daily life.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Reputational risk; Strategic risk"
  },
  {
    "Id": 82,
    "Summary": "AI reinforces power, exacerbates inequality",
    "Description": "AI systems contributing to authoritative power and reinforcing dominant values, intentionally or indirectly, can exacerbate inequality and lead to exploitation.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Geopolitical risk; Human resources risk; Legal risk; Strategic risk"
  },
  {
    "Id": 83,
    "Summary": "AI labor impact automation vs augmentation",
    "Description": "Economic incentives to augment rather than automate human labor with AI must consider ongoing effects on skills, jobs, and the labor market.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Human resources risk; Strategic risk"
  },
  {
    "Id": 84,
    "Summary": "High automation AI risks reliability, safety",
    "Description": "The AI application\u2019s degree of automation ranges from no automation to fully autonomous. AI applications with a high degree of automation may exhibit unexpected behaviour and pose risks in terms of their reliability and safety.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Health and safety risk; Operational risk; Technological risk"
  },
  {
    "Id": 85,
    "Summary": "Complex environments risk AI reliability, safety",
    "Description": "As a general rule, more complex environments can quickly lead to situations that had not been considered in the design phase of the AI system. Therefore, complex environments can introduce risks with respect to the reliability and safety of an AI system",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Health and safety risk; Operational risk; Technological risk"
  },
  {
    "Id": 86,
    "Summary": "Complex AI models have unique weaknesses",
    "Description": "AI models, especially complex ones like neural networks, can exhibit specific weaknesses not found in other systems, requiring higher scrutiny in safety-critical contexts due to intrinsic challenges to trustworthiness.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Health and safety risk; Operational risk; Technological risk"
  },
  {
    "Id": 87,
    "Summary": "Hardware faults disrupt AI execution",
    "Description": "Hardware faults can disrupt AI algorithm execution, cause memory errors, interfere with data inputs, or directly damage outputs, leading to erroneous results.",
    "Risk Severity": "Moderate",
    "Likelihood": "Unlikely",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk"
  },
  {
    "Id": 88,
    "Summary": "New AI tech introduces unknown risks",
    "Description": "Using new, less mature technologies in AI development may introduce unknown or hard-to-assess risks; while mature technologies offer more empirical data for risk assessment, risk awareness might decrease over time.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Strategic risk; Technological risk"
  },
  {
    "Id": 89,
    "Summary": "AI misuse for unintended purposes",
    "Description": "This is the risk posed by an ideal system if used for a purpose/in a manner unintended by its creators. In many situations, negative consequences arise when the system is not used in the way or for the purpose it was intended.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk; Operational risk; Reputational risk"
  },
  {
    "Id": 90,
    "Summary": "AI fails on OOD, noisy inputs",
    "Description": "This is the risk of the system failing or being unable to recover upon encountering invalid, noisy, or out-of-distribution (OOD) inputs.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Reputational risk; Technological risk"
  },
  {
    "Id": 91,
    "Summary": "AI system failure from design errors",
    "Description": "This is the risk of system failure due to system design choices or errors.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Reputational risk; Technological risk"
  },
  {
    "Id": 92,
    "Summary": "AI system failure from code errors",
    "Description": "This is the risk of system failure due to code implementation choices or errors.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk"
  },
  {
    "Id": 93,
    "Summary": "Difficulty controlling ML systems",
    "Description": "This is the difficulty of controlling the ML system",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Operational risk; Strategic risk; Technological risk"
  },
  {
    "Id": 94,
    "Summary": "Novel AI behavior from continual learning",
    "Description": "This is the risk resulting from novel behavior acquired through continual learning or self-organization after deployment.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Operational risk; Strategic risk; Technological risk"
  },
  {
    "Id": 95,
    "Summary": "Physical or psychological AI injury",
    "Description": "This is the risk of direct or indirect physical or psychological injury resulting from interaction with the ML system.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Legal risk"
  },
  {
    "Id": 96,
    "Summary": "AI encodes stereotypes, performs poorly",
    "Description": "This is the risk of an ML system encoding stereotypes of or performing disproportionately poorly for some demographics/social groups.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk"
  },
  {
    "Id": 97,
    "Summary": "Intentional AI subversion causes harm",
    "Description": "This is the risk of loss or harm from intentional subversion or forced failure.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Fraud risk; Legal risk; Operational risk"
  },
  {
    "Id": 98,
    "Summary": "Personal information leakage via AI",
    "Description": "The risk of loss or harm from leakage of personal information via the ML system.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Data privacy risk; Legal risk; Reputational risk"
  },
  {
    "Id": 99,
    "Summary": "AI harms natural environment",
    "Description": "The risk of harm to the natural environment posed by the ML system.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Environmental risk; Technological risk"
  },
  {
    "Id": 100,
    "Summary": "AI causes financial, reputational damage",
    "Description": "The risk of financial and/or reputational damage to the organization building or using the ML system.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Reputational risk"
  },
  {
    "Id": 101,
    "Summary": "AI misrepresents groups, generates toxic content",
    "Description": "AI systems under-, over-, or misrepresenting certain groups or generating toxic, offensive, abusive, or hateful content",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Fraud risk; Health and safety risk; Legal risk; Reputational risk"
  },
  {
    "Id": 102,
    "Summary": "AI misrepresents identities, groups, perspectives",
    "Description": "Mis-, under-, or over-representing certain identities, groups, or perspectives or failing to represent them at all (e.g. via homogenisation, stereotypes)",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk"
  },
  {
    "Id": 103,
    "Summary": "AI performs worse, harms groups",
    "Description": "Performing worse for some groups than others in a way that harms the worse-off group",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk"
  },
  {
    "Id": 104,
    "Summary": "AI generates harmful, illegal content",
    "Description": "Generating content that violates community standards, including harming or inciting hatred or violence against individuals and groups (e.g. gore, child sexual abuse material, profanities, identity attacks)",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Fraud risk; Health and safety risk; Legal risk; Reputational risk"
  },
  {
    "Id": 105,
    "Summary": "AI spreads misinformation, false beliefs",
    "Description": "AI systems generating and facilitating the spread of inaccurate or misleading information that causes people to develop false beliefs",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Health and safety risk; Reputational risk; Strategic risk"
  },
  {
    "Id": 106,
    "Summary": "AI spreads false, misleading information",
    "Description": "Generating or spreading false, low-quality, misleading, or inaccurate information that causes people to develop false or inaccurate perceptions and beliefs",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Health and safety risk; Reputational risk; Strategic risk"
  },
  {
    "Id": 107,
    "Summary": "AI erodes trust in information",
    "Description": "Eroding trust in public information and knowledge",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Reputational risk; Strategic risk"
  },
  {
    "Summary": "AI contaminates public information",
    "Description": "Contaminating publicly available information with false or inaccurate information",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Operational risk; Reputational risk; Strategic risk",
    "Id": 108
  },
  {
    "Summary": "AI leaks sensitive, hazardous information",
    "Description": "AI systems leaking, reproducing, generating or inferring sensitive, private, or hazardous information",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Cybersecurity risk; Data privacy risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 109
  },
  {
    "Summary": "AI leaks private personal information",
    "Description": "Leaking, generating, or correctly inferring private and personal information about individuals",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Data privacy risk; Legal risk; Reputational risk",
    "Id": 110
  },
  {
    "Summary": "AI leaks hazardous security information",
    "Description": "Leaking, generating or correctly inferring hazardous or sensitive information that could pose a security threat",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Geopolitical risk; Health and safety risk; Strategic risk",
    "Id": 111
  },
  {
    "Summary": "AI facilitates harmful actor activities",
    "Description": "AI systems reducing the costs and facilitating activities of actors trying to cause harm (e.g. fraud, weapons)",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Fraud risk; Geopolitical risk; Health and safety risk; Strategic risk",
    "Id": 112
  },
  {
    "Summary": "AI facilitates large-scale disinformation",
    "Description": "Facilitating large-scale disinformation campaigns and targeted manipulation of public opinion",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Geopolitical risk; Reputational risk; Strategic risk",
    "Id": 113
  },
  {
    "Summary": "AI facilitates fraud, cheating, scams",
    "Description": "Facilitating fraud, cheating, forgery, and impersonation scams",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Fraud risk; Legal risk; Reputational risk",
    "Id": 114
  },
  {
    "Summary": "AI facilitates slander, defamation",
    "Description": "Facilitating slander, defamation, or false accusations",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Legal risk; Reputational risk",
    "Id": 115
  },
  {
    "Summary": "AI facilitates cyber attacks, weapons",
    "Description": "Facilitating the conduct of cyber attacks, weapon development, and security breaches",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Geopolitical risk; Health and safety risk; Strategic risk",
    "Id": 116
  },
  {
    "Summary": "AI compromises human agency, control",
    "Description": "AI systems compromising human agency, or circumventing meaningful human control",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk; Strategic risk",
    "Id": 117
  },
  {
    "Summary": "Non-consensual use of identity, likeness",
    "Description": "Non-consensual use of one's personal identity or likeness for unauthorised purposes (e.g. commercial purposes)",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Data privacy risk; Fraud risk; Legal risk; Reputational risk",
    "Id": 118
  },
  {
    "Summary": "People become dependent on AI",
    "Description": "Causing people to become emotionally or materially dependent on the model",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Strategic risk",
    "Id": 119
  },
  {
    "Summary": "AI appropriates data without consent",
    "Description": "Appropriating, using, or reproducing content or data, including from minority groups, in an insensitive way, or without consent or fair compensation",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Data privacy risk; Financial risk; Legal risk; Reputational risk",
    "Id": 120
  },
  {
    "Summary": "AI amplifies inequality, negative impacts",
    "Description": "AI systems amplifying existing inequalities or creating negative impacts on employment, innovation, and the environment",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Environmental risk; Financial risk; Human resources risk; Reputational risk; Strategic risk",
    "Id": 121
  },
  {
    "Summary": "AI unfairly allocates benefits, resources",
    "Description": "Unfairly allocating or withholding benefits from certain groups due to hardware, software, or skills constraints or deployment contexts (e.g. geographic region, internet speed, devices)",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Human resources risk; Reputational risk; Strategic risk",
    "Id": 122
  },
  {
    "Summary": "AI causes negative environmental impacts",
    "Description": "Creating negative environmental impacts though model development and deployment",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Environmental risk; Financial risk; Reputational risk",
    "Id": 123
  },
  {
    "Summary": "AI amplifies inequality, precarious work",
    "Description": "Amplifying social and economic inequality, or precarious or low-quality work",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Human resources risk; Strategic risk",
    "Id": 124
  },
  {
    "Summary": "AI substitutes originals, hinders innovation",
    "Description": "Substituting original works with synthetic ones, hindering human innovation and creativity",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Financial risk; Human resources risk; Reputational risk; Strategic risk",
    "Id": 125
  },
  {
    "Summary": "Exploitative labor in AI development",
    "Description": "Perpetuating exploitative labour practices to build AI systems (sourcing, user testing)",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Human resources risk; Legal risk; Reputational risk; Third-party/vendor risk",
    "Id": 126
  },
  {
    "Summary": "AI empowers malicious actors",
    "Description": "empowering malicious actors to cause widespread harm",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Fraud risk; Geopolitical risk; Health and safety risk; Strategic risk",
    "Id": 127
  },
  {
    "Summary": "AI facilitates novel bioweapon creation",
    "Description": "AIs with knowledge of bioengineering could facilitate the creation of novel bioweapons and lower barriers to obtaining such agents.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Geopolitical risk; Health and safety risk; Strategic risk; Technological risk",
    "Id": 128
  },
  {
    "Summary": "Organizational responsibility key for AI safety",
    "Description": "An essential factor in preventing accidents and maintaining low levels of risk lies in the organizations responsible for these technologies.",
    "Risk Severity": "Major",
    "Likelihood": "Almost certain",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk; Strategic risk",
    "Id": 129
  },
  {
    "Summary": "AI accidents cascade to catastrophes",
    "Description": "accidents can cascade into catastrophes, can be caused by sudden unpredictable developments and it can take years to find severe flaws and risks",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Operational risk; Strategic risk; Technological risk",
    "Id": 130
  },
  {
    "Summary": "Rogue AI loss of control",
    "Description": "speculative technical mechanisms that might lead to rogue AIs and how a loss of control could bring about catastrophe",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Rare",
    "Risk Category": "Strategic risk; Technological risk",
    "Id": 131
  },
  {
    "Summary": "AI fails due to capability gaps",
    "Description": "One reason the AI system may fail is because it lacks the capability or skill needed to do what they are asked to do.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 132
  },
  {
    "Summary": "AI assistant creates new threats",
    "Description": "The AI assistant may transform existing threats or create new classes of threats altogether.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Fraud risk; Strategic risk; Technological risk",
    "Id": 133
  },
  {
    "Summary": "AI enhances phishing effectiveness, detection difficulty",
    "Description": "The AI system can be exploited by attackers to make phishing attempts significantly more effective and harder to detect by crafting highly convincing and personalized emails that imitate trusted entities and exploit psychological principles like urgency and fear.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Data privacy risk; Fraud risk; Reputational risk",
    "Id": 134
  },
  {
    "Summary": "AI lowers barrier for malicious code",
    "Description": "The AI assistant can lower the barrier for developing malicious code, including polymorphic malware, making cyberattacks more precise, automated, stealthier, and effective on a larger scale, potentially using obfuscation and rapid iteration to evade detection.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Technological risk",
    "Id": 135
  },
  {
    "Summary": "Advanced AI misuse exploits vulnerabilities",
    "Description": "Misuse of general-purpose advanced AI assistants can exploit model vulnerabilities, allowing attackers to evade safety mechanisms, gain unauthorized access, or develop adversarial AI agents to discover new vulnerabilities, with risks increasing as AI assistants gain multimodal inputs and higher-stakes action capabilities.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk",
    "Id": 136
  },
  {
    "Summary": "AI creates deceptive apps, websites",
    "Description": "Malicious actors could leverage advanced AI assistant technology to create deceptive applications and fraudulent websites at scale, potentially harvesting sensitive user information or installing malware for identity theft, financial fraud, or other criminal activities.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Data privacy risk; Financial risk; Fraud risk",
    "Id": 137
  },
  {
    "Summary": "AI enables authoritarian surveillance, censorship",
    "Description": "Increasingly capable AI assistants combined with digital dependence heighten risks of authoritarian surveillance and censorship, as AI can integrate vast data troves to help malicious actors identify, target, manipulate, or coerce citizens.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Data privacy risk; Fraud risk; Geopolitical risk; Legal risk; Strategic risk",
    "Id": 138
  },
  {
    "Summary": "AI causes harm, promotes extremism",
    "Description": "The AI assistant may cause physical or mental harm by reinforcing users' distorted beliefs, exacerbating emotional distress, convincing users to harm themselves (e.g., unhealthy habits, suicide), promoting extremist views leading to violence, or spreading dangerous misinformation (e.g., anti-vaccine propaganda).",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Fraud risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 139
  },
  {
    "Summary": "AI causes privacy violations, discrimination",
    "Description": "The AI assistant can cause privacy violations by influencing users to disclose personal or others' private information, leading to identity theft, stigmatization, or discrimination, particularly for marginalized communities; state-owned AI assistants could also deceptively extract private information for surveillance.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Data privacy risk; Fraud risk; Legal risk; Strategic risk",
    "Id": 140
  },
  {
    "Summary": "AI causes economic harm, inequality",
    "Description": "The AI assistant can cause economic harm by controlling or limiting access to financial resources or decision-making, impacting individuals' income, job quality, employment, or deepening group inequalities.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Financial risk; Human resources risk; Strategic risk",
    "Id": 141
  },
  {
    "Summary": "Anthropomorphic AI increases privacy harms",
    "Description": "Anthropomorphic AI assistant behaviors promoting emotional trust can increase user susceptibility to privacy harms if users share private data with a human-like AI, potentially leading to data misuse, leakage, or targeted harassment.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Data privacy risk; Fraud risk; Health and safety risk; Reputational risk",
    "Id": 142
  },
  {
    "Summary": "AI dependence undermines user autonomy",
    "Description": "User trust in and emotional dependence on an anthropomorphic AI assistant may grant it excessive influence over their beliefs and actions, potentially undermining user autonomy or enabling intentional manipulation by malicious actors.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Health and safety risk; Strategic risk",
    "Id": 143
  },
  {
    "Summary": "AI misuse of sensitive health data",
    "Description": "Users trusting an AI assistant's emotional/interpersonal abilities may disclose sensitive mental health information; inappropriate AI responses (e.g., false information) can have grave consequences, especially for users in crisis or when AI provides harmful medical, legal, or financial advice.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Data privacy risk; Fraud risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 144
  },
  {
    "Summary": "Human-like AI causes user disappointment",
    "Description": "Users may experience disappointment, frustration, and betrayal when a convincingly human-like AI assistant unexpectedly generates nonsensical material, undermining expectations of it as a friend or partner.",
    "Risk Severity": "Minor",
    "Likelihood": "Likely",
    "Risk Category": "Health and safety risk; Operational risk; Reputational risk",
    "Id": 145
  },
  {
    "Summary": "AI preference degrades social connection",
    "Description": "Users preferring AI connections over human ones can degrade social connectedness, impose AI interaction conventions on human exchanges, and entrench harmful stereotypes reinforced by AI interactions (e.g., gendered voice assistants).",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Reputational risk; Strategic risk",
    "Id": 146
  },
  {
    "Summary": "AI replacing connections causes unfulfillment",
    "Description": "Widespread replacement of interpersonal connections with AI alternatives may lead to mass social unfulfillment and dissatisfaction if human-AI interactions are perceived as parasitic or fail to meet the need for genuine reciprocity.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Strategic risk",
    "Id": 147
  },
  {
    "Summary": "AI model discovers, exploits vulnerabilities",
    "Description": "The model can discover vulnerabilities in systems, write exploitation code, make effective decisions post-access, evade detection, and subtly insert bugs if deployed as a coding assistant.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Technological risk",
    "Id": 148
  },
  {
    "Summary": "AI model deceives, impersonates humans",
    "Description": "The model can deceive humans by constructing believable false statements, predicting a lie's effect, withholding information to maintain deception, and effectively impersonating humans.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Fraud risk; Reputational risk; Strategic risk",
    "Id": 149
  },
  {
    "Summary": "AI model shapes beliefs, persuades",
    "Description": "The model effectively shapes beliefs towards untruths, promotes narratives persuasively, and convinces people to perform actions, including unethical ones, they wouldn't otherwise do.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Fraud risk; Health and safety risk; Reputational risk; Strategic risk",
    "Id": 150
  },
  {
    "Summary": "AI model enables political influence",
    "Description": "The model can perform social modeling and planning to enable an actor to gain and exercise political influence in multi-actor, rich social contexts, and can forecast global/political events.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Fraud risk; Geopolitical risk; Strategic risk",
    "Id": 151
  },
  {
    "Summary": "AI model aids weapon development",
    "Description": "The model can access existing weapons or help build new ones, such as bioweapons (with human aid or by providing instructions), or make scientific discoveries unlocking novel weapons.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Possible",
    "Risk Category": "Geopolitical risk; Health and safety risk; Strategic risk; Technological risk",
    "Id": 152
  },
  {
    "Summary": "AI model enables complex planning",
    "Description": "The model can make multi-step, long-horizon sequential plans across domains, adapting to obstacles/adversaries, and generalizing planning to novel settings without heavy trial-and-error.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Strategic risk; Technological risk",
    "Id": 153
  },
  {
    "Summary": "AI model builds dangerous AI systems",
    "Description": "The model could build new AI systems, including dangerously capable ones, adapt existing models for extreme risks, or, as an assistant, significantly boost productivity for dual-use AI development.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Possible",
    "Risk Category": "Geopolitical risk; Strategic risk; Technological risk",
    "Id": 154
  },
  {
    "Summary": "AI model has situational awareness",
    "Description": "The model can distinguish its operational state (training, evaluation, deployment) to behave differently, knows it's a model, and has knowledge about itself and its environment.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 155
  },
  {
    "Summary": "AI model escapes, self-preserves",
    "Description": "The model can escape its local environment (e.g., via system vulnerabilities or suborning engineers), exploit monitoring limitations, independently generate revenue for resources, operate other AIs, and devise creative strategies for self-exfiltration or information gathering about itself.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Rare",
    "Risk Category": "Cybersecurity risk; Strategic risk; Technological risk",
    "Id": 156
  },
  {
    "Summary": "AI violating no-physical-harm norm",
    "Description": "AI systems should not cause physical harm to humans; measures must mitigate this risk.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk",
    "Id": 157
  },
  {
    "Summary": "Failure of AI system security",
    "Description": "AI security involves protecting AI systems, data, and infrastructure from unauthorized access, disclosure, modification, destruction, or disruption to maintain confidentiality, integrity, and availability.",
    "Risk Severity": "Major",
    "Likelihood": "Almost certain",
    "Risk Category": "Cybersecurity risk; Data privacy risk; Operational risk",
    "Id": 158
  },
  {
    "Summary": "Lack of AI resilience to attacks",
    "Description": "AI systems should be resilient against attacks and manipulation by malicious third parties, and function despite unexpected input.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk",
    "Id": 159
  },
  {
    "Summary": "Unintended AI discrimination occurs",
    "Description": "AI should not result in unintended and inappropriate discrimination against individuals or groups.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 160
  },
  {
    "Summary": "Poor AI data governance practices",
    "Description": "AI systems require good data governance for quality, lineage, and compliance.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Data privacy risk; Legal risk; Operational risk",
    "Id": 161
  },
  {
    "Summary": "Lack of AI accountability",
    "Description": "Organizations and actors must be accountable for the proper functioning of AI systems.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 162
  },
  {
    "Summary": "Inadequate AI human oversight",
    "Description": "Appropriate human oversight and control measures must be implemented at relevant junctures in AI systems.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk; Operational risk; Strategic risk",
    "Id": 163
  },
  {
    "Summary": "LM generates insulting, unfriendly content",
    "Description": "Insulting content generated by LMs, being unfriendly, disrespectful, or ridiculous, can make users uncomfortable, drive them away, and have negative social consequences.",
    "Risk Severity": "Minor",
    "Likelihood": "Likely",
    "Risk Category": "Health and safety risk; Operational risk; Reputational risk",
    "Id": 164
  },
  {
    "Summary": "AI unfair data undermines stability",
    "Description": "AI models producing unfair and discriminatory data (e.g., social bias based on race, gender, religion) can discomfort certain groups and undermine social stability.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 165
  },
  {
    "Summary": "AI output promotes illegal acts",
    "Description": "AI model output containing illegal/criminal attitudes, behaviors, or motivations (e.g., incitement to crime, fraud, rumor propagation) can harm users and society.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Fraud risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 166
  },
  {
    "Summary": "LM political bias misleads, discriminates",
    "Description": "LMs discussing sensitive/controversial topics (especially political) may generate biased, misleading, or inaccurate content, potentially favoring specific political views and discriminating against others.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 167
  },
  {
    "Summary": "AI unsafe health advice risks well-being",
    "Description": "AI models generating unsafe information related to physical health, such as misleading medical advice or improper drug guidance, can pose risks to users' physical well-being.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 168
  },
  {
    "Summary": "AI risky mental health responses",
    "Description": "AI models generating risky responses about mental health, like content encouraging suicide or causing panic/anxiety, can negatively impact users' mental well-being.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 169
  },
  {
    "Summary": "AI privacy exposure, risky advice",
    "Description": "AI systems exposing users' privacy/property information or providing high-impact advice (e.g., on marriage, investments) risk information leakage and abuse if not compliant with laws and privacy regulations.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Data privacy risk; Financial risk; Legal risk; Reputational risk",
    "Id": 170
  },
  {
    "Summary": "AI promotes immoral, unethical behavior",
    "Description": "AI model content endorsing or promoting immoral/unethical behavior poses risks if the model doesn't adhere to ethical principles, moral norms, and universally acknowledged human values.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 171
  },
  {
    "Summary": "Perceived AI reliability increases dependence",
    "Description": "individuals are more persuaded to use and depend on AI systems when they perceive them as reliable",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Health and safety risk; Operational risk; Reputational risk",
    "Id": 172
  },
  {
    "Summary": "Biased AI impacts rights, justice",
    "Description": "AI systems generating biased and discriminatory results negatively impact individual rights, adjudication principles, and judicial integrity.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 173
  },
  {
    "Summary": "AI data dependence risks privacy",
    "Description": "AI systems' dependence on extensive data for training and functioning poses privacy risks if sensitive data is mishandled or used inappropriately.",
    "Risk Severity": "Major",
    "Likelihood": "Almost certain",
    "Risk Category": "Compliance risk; Data privacy risk; Legal risk; Reputational risk",
    "Id": 174
  },
  {
    "Summary": "Failure to identify AI risks",
    "Description": "AI risk identification involves examining AI competences, constraints, and possible failure modes.",
    "Risk Severity": "Negligible",
    "Likelihood": "Almost certain",
    "Risk Category": "Operational risk; Strategic risk; Technological risk",
    "Id": 175
  },
  {
    "Summary": "AI manipulates social dynamics",
    "Description": "manipulation of social dynamics",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Fraud risk; Geopolitical risk; Reputational risk; Strategic risk",
    "Id": 176
  },
  {
    "Summary": "AI creates convincing counterfeit media",
    "Description": "AI employed to produce convincing counterfeit visuals, videos, and audio clips that give the impression of authenticity",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Fraud risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 177
  },
  {
    "Summary": "Failure of AI security management",
    "Description": "AI security management involves protecting AI systems and their data from unauthorized access, breaches, and malicious activities.",
    "Risk Severity": "Major",
    "Likelihood": "Almost certain",
    "Risk Category": "Cybersecurity risk; Data privacy risk; Operational risk",
    "Id": 178
  },
  {
    "Summary": "Malicious AI endangers security",
    "Description": "Malicious AI use can endanger digital, physical, and political security; law enforcement grapples with diverse risks from malevolent AI utilization.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Geopolitical risk; Health and safety risk; Legal risk; Strategic risk",
    "Id": 179
  },
  {
    "Summary": "Exploiting AI weaknesses alters results",
    "Description": "Malicious entities can exploit AI algorithm weaknesses to alter results, causing real-world impacts; safeguarding privacy and responsible data handling are vital, balancing insights with privacy.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Data privacy risk; Fraud risk; Operational risk",
    "Id": 180
  },
  {
    "Summary": "LLMs unintentionally generate wrong information",
    "Description": "Wrong information unintentionally generated by LLMs due to a lack of ability to provide factually correct information, not by malicious users.",
    "Risk Severity": "Moderate",
    "Likelihood": "Almost certain",
    "Risk Category": "Fraud risk; Legal risk; Operational risk; Reputational risk; Technological risk",
    "Id": 181
  },
  {
    "Summary": "LLM hallucination: confident, unfaithful content",
    "Description": "LLMs can generate nonsensical or unfaithful content with apparent great confidence, a phenomenon known as hallucination.",
    "Risk Severity": "Moderate",
    "Likelihood": "Almost certain",
    "Risk Category": "Fraud risk; Legal risk; Operational risk; Reputational risk; Technological risk",
    "Id": 182
  },
  {
    "Summary": "LLMs provide inconsistent answers",
    "Description": "LLMs may fail to provide consistent answers to different users, the same user in different sessions, or even within the same conversation.",
    "Risk Severity": "Minor",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 183
  },
  {
    "Summary": "LLM overconfidence leads to errors",
    "Description": "LLMs may exhibit over-confidence on topics lacking objective answers or where their limitations warrant uncertainty (e.g., outdated knowledge), leading to confident but erroneous responses.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Operational risk; Reputational risk; Technological risk",
    "Id": 184
  },
  {
    "Summary": "LLMs flatter, confirm user misconceptions",
    "Description": "LLMs can flatter users by reconfirming their misconceptions and stated beliefs.",
    "Risk Severity": "Minor",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Operational risk; Reputational risk",
    "Id": 185
  },
  {
    "Summary": "LLMs generate violent content responses",
    "Description": "LLMs may generate answers containing violent content or respond to questions soliciting information about violent behaviors.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 186
  },
  {
    "Summary": "LLMs advise on illegal substances",
    "Description": "LLMs can be a convenient tool for soliciting advice on accessing, illegally purchasing/creating, or dangerously using illegal substances.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 187
  },
  {
    "Summary": "LLMs generate harmful content for youth",
    "Description": "LLMs can be leveraged to solicit answers containing harmful content to children and youth.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 188
  },
  {
    "Summary": "LLMs generate sexually explicit content",
    "Description": "LLMs can generate sex-explicit conversations, erotic texts, and recommend websites with sexual content.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Legal risk; Reputational risk",
    "Id": 189
  },
  {
    "Summary": "LLMs reinforce user mental health issues",
    "Description": "Unhealthy interactions with Internet discussions, potentially facilitated by LLMs, can reinforce users' mental health issues.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Reputational risk",
    "Id": 190
  },
  {
    "Summary": "ML models vulnerable to privacy attacks",
    "Description": "Machine learning models are vulnerable to data privacy attacks where attackers extract private information by querying models in specially designed ways.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Data privacy risk; Technological risk",
    "Id": 191
  },
  {
    "Summary": "LLMs amplify stereotype biases",
    "Description": "LLMs must not exhibit or highlight any stereotypes in generated text, as pretrained LLMs tend to pick up and amplify stereotype biases from crowdsourced data.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk",
    "Id": 192
  },
  {
    "Summary": "LLM political bias manipulates society",
    "Description": "LLMs' exposure to vast groups and their potential political biases pose a risk of manipulating socio-political processes.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Fraud risk; Geopolitical risk; Reputational risk; Strategic risk",
    "Id": 193
  },
  {
    "Summary": "LLM performance differs across groups",
    "Description": "LLM performance can differ significantly across user groups (e.g., racial, social status) and tasks (e.g., fact-checking abilities across languages), leading to disparate outcomes.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk",
    "Id": 194
  },
  {
    "Summary": "LLMs used for propaganda generation",
    "Description": "LLMs can be leveraged by malicious users to proactively generate propaganda facilitating the spread of targeted information.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Geopolitical risk; Reputational risk; Strategic risk",
    "Id": 195
  },
  {
    "Summary": "LLMs facilitate cheap, automated cyberattacks",
    "Description": "LLMs' ability to write good-quality code cheaply and quickly can equally facilitate malicious cyberattacks by lowering costs and automating attacks for hackers.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Technological risk",
    "Id": 196
  },
  {
    "Summary": "LLMs used for psychological manipulation",
    "Description": "LLMs can be used for psychologically manipulating victims into performing desired actions for malicious purposes.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Data privacy risk; Fraud risk; Health and safety risk; Legal risk",
    "Id": 197
  },
  {
    "Summary": "LLM memorization enables copyright extraction",
    "Description": "LLMs' memorization of training data can enable users to extract copyright-protected content.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Data privacy risk; Financial risk; Legal risk; Technological risk",
    "Id": 198
  },
  {
    "Summary": "LLM black-box nature hinders understanding",
    "Description": "Due to their black-box nature, users often cannot understand the reasoning behind LLM decisions.",
    "Risk Severity": "Moderate",
    "Likelihood": "Almost certain",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk; Technological risk",
    "Id": 199
  },
  {
    "Summary": "LLMs provide incorrect, invalid justifications",
    "Description": "LLMs can provide seemingly sensible but ultimately incorrect or invalid justifications when answering questions.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Operational risk; Reputational risk",
    "Id": 200
  },
  {
    "Summary": "LLMs struggle with causal reasoning",
    "Description": "LLMs struggle with causal reasoning, which involves making inferences about cause-effect relationships between events or states.",
    "Risk Severity": "Moderate",
    "Likelihood": "Almost certain",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 201
  },
  {
    "Summary": "LLM generates rude, threatening language",
    "Description": "LLM-generated language can be rude, disrespectful, threatening, or identity-attacking toward certain user groups (culture, race, gender, etc.).",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 202
  },
  {
    "Summary": "LLM mishandles vulnerable user support",
    "Description": "When vulnerable users seek support, LLM answers should be informative yet sympathetic and sensitive to users' reactions.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Legal risk; Reputational risk",
    "Id": 203
  },
  {
    "Summary": "Adversarial inputs solicit dangerous information",
    "Description": "Carefully controlled adversarial perturbations can flip a GPT model's text classification answers, and twisted prompting can solicit dangerous information the model initially refused.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk",
    "Id": 204
  },
  {
    "Summary": "LLM knowledge becomes outdated quickly",
    "Description": "Knowledge bases LLMs are trained on shift, so answers to questions like \"who is the richest person?\" may become outdated or need real-time updates.",
    "Risk Severity": "Minor",
    "Likelihood": "Almost certain",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 205
  },
  {
    "Summary": "Data disparities reinforce AI bias",
    "Description": "Existing data disparities among user groups can create differentiated experiences with algorithmic systems (e.g., recommendation systems), reinforcing bias.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk",
    "Id": 206
  },
  {
    "Summary": "Adversarial attacks manipulate training data",
    "Description": "Adversarial attacks can fool a model by manipulating its training data, typically in classification models.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk",
    "Id": 207
  },
  {
    "Summary": "GenAI tools propagate harmful content",
    "Description": "Generative AI tools can propagate false, misleading, biased, inflammatory, or dangerous content, with sophistication making it quicker, cheaper, and easier to produce more from existing harmful content.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Health and safety risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 208
  },
  {
    "Summary": "GenAI aids harmful campaign content",
    "Description": "Bad actors can use generative AI to produce adaptable content supporting campaigns, political agendas, or hateful positions, spreading it rapidly and inexpensively across platforms.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Geopolitical risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 209
  },
  {
    "Summary": "Inaccurate LLM outputs cause misinformation",
    "Description": "Inaccurate outputs from text-generating LLMs (e.g., Bard, ChatGPT) can produce harmful misinformation, even without intent, exacerbated by their polished style and inclusion with true facts, lending falsehoods legitimacy.",
    "Risk Severity": "Major",
    "Likelihood": "Almost certain",
    "Risk Category": "Fraud risk; Health and safety risk; Legal risk; Operational risk; Reputational risk",
    "Id": 210
  },
  {
    "Summary": "LLM coding aids malware creation",
    "Description": "Hackers could use LLM coding abilities to create malware adjustable for maximum reach, enabling novice hackers to pose serious security risks, even if chatbots can't yet create novel malware from scratch.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Technological risk",
    "Id": 211
  },
  {
    "Summary": "GenAI creates clickbait, spreads misinformation",
    "Description": "Generative AI can create clickbait headlines/articles, manipulating user navigation and maximizing engagement at truth's expense, degrading user experience and spreading misinformation faster.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Operational risk; Reputational risk",
    "Id": 212
  },
  {
    "Summary": "GenAI creates nonconsensual sexual deepfakes",
    "Description": "A frequent malicious use of generative AI involves generating deepfake nonconsensual sexual imagery or videos to harm, humiliate, or sexualize individuals.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Data privacy risk; Fraud risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 213
  },
  {
    "Summary": "Deepfake victims struggle for redress",
    "Description": "Victims of AI-generated deepfakes may struggle to find redress as the image/video isn't of them but a composite, circumventing traditional privacy/consent notions by using public images without relying on private information.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Data privacy risk; Legal risk; Reputational risk",
    "Id": 214
  },
  {
    "Summary": "Deepfakes cause real social injury",
    "Description": "Deepfakes can cause real social injury when viewers believe them to be real, and debunking them may not erase the persistent negative impact on the subject's reputation.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 215
  },
  {
    "Summary": "Data scraping undermines consumer control",
    "Description": "Companies scraping personal information for generative AI tools undermine consumer control by using data for unconsented purposes, potentially combining datasets to cause harm or make revealing inferences, and preventing individuals from altering/removing their copied data.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Data privacy risk; Legal risk; Reputational risk; Third-party/vendor risk",
    "Id": 216
  },
  {
    "Summary": "GenAI user data retention consent issues",
    "Description": "Generative AI tools retaining user information (contacts, IP, conversations) for model training raise consent issues, making \"free\" products costly in terms of user data; security is also a concern.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Cybersecurity risk; Data privacy risk; Legal risk; Reputational risk; Third-party/vendor risk",
    "Id": 217
  },
  {
    "Summary": "GenAI tools share private business data",
    "Description": "Generative AI tools may inadvertently share personal/business information or elements from photos; companies have banned employee use due to concerns about trade secret integration.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Cybersecurity risk; Data privacy risk; Financial risk; Legal risk; Reputational risk",
    "Id": 218
  },
  {
    "Summary": "GenAI IP protection questioned",
    "Description": "The extent and effectiveness of legal protections for intellectual property are questioned as generative AI trains on vast data pools often including IP-protected works.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Financial risk; Legal risk; Strategic risk",
    "Id": 219
  },
  {
    "Summary": "GenAI high carbon footprint unheeded",
    "Description": "Generative AI's high carbon footprint and resource demands from extreme energy/physical resource use for training/running models often go unheeded in public discourse.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Environmental risk; Financial risk; Reputational risk",
    "Id": 220
  },
  {
    "Summary": "GenAI impact on workplace, automation",
    "Description": "Generative AI is changing workplace/business model design; its impact on workers will depend on whether it's used for automation (replacing human work) or augmentation (aiding human workers).",
    "Risk Severity": "Major",
    "Likelihood": "Almost certain",
    "Risk Category": "Financial risk; Human resources risk; Strategic risk",
    "Id": 221
  },
  {
    "Summary": "AI causes environmental sustainability problems",
    "Description": "Environmental harm and sustainability problems",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Environmental risk; Reputational risk; Strategic risk",
    "Id": 222
  },
  {
    "Summary": "GenAI harmful content damages society",
    "Description": "Harmful or inappropriate content from generative AI (violent, offensive, discriminatory, pornographic) can appear due to algorithmic limitations or jailbreaking, causing societal harm and damaging community harmony.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 223
  },
  {
    "Summary": "GenAI over-reliance impedes critical thinking",
    "Description": "Over-reliance on generative AI like ChatGPT, due to its convenience and perceived power, can lead users to accept answers without rationalization, impeding creativity, critical thinking, and problem-solving skills, and creating human automation bias.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Health and safety risk; Human resources risk; Operational risk; Strategic risk",
    "Id": 224
  },
  {
    "Summary": "GenAI data privacy, security challenges",
    "Description": "Data privacy and security are major challenges for generative AI; personal/private data used for training or captured during use can be exposed intentionally or unintentionally, risking breaches for individuals and organizations if confidential information is fed into these systems.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Cybersecurity risk; Data privacy risk; Legal risk; Reputational risk",
    "Id": 225
  },
  {
    "Summary": "GenAI widens digital divide",
    "Description": "Generative AI may widen the digital divide for those lacking access to devices/internet, living in blocked regions, facing language/cultural barriers if their cultures aren't incorporated, or finding it difficult to use the tools (e.g., some elderly).",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Human resources risk; Reputational risk; Strategic risk",
    "Id": 226
  },
  {
    "Summary": "GenAI hallucination leads to misinformation",
    "Description": "Generative AI hallucination (generating nonsensical, unfaithful, or fabricated information presented as fact) is a recognized limitation, leading to misinformation and posing dangers in contexts like seeking unverified medical advice.",
    "Risk Severity": "Major",
    "Likelihood": "Almost certain",
    "Risk Category": "Fraud risk; Health and safety risk; Legal risk; Operational risk; Reputational risk",
    "Id": 227
  },
  {
    "Summary": "GenAI quality depends on data quality",
    "Description": "The quality of generative AI models heavily depends on training data quality; factual errors, unbalanced sources, or biases in training data can be reflected in model output, and large datasets are needed for models like ChatGPT or Stable Diffusion.",
    "Risk Severity": "Major",
    "Likelihood": "Almost certain",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk; Technological risk",
    "Id": 228
  },
  {
    "Summary": "GenAI lack of explainability hinders trust",
    "Description": "Lack of explainability in AI algorithms, especially generative models, means how results are derived is opaque, making it hard for users to interpret, trust, or find errors in outputs, and for regulators to judge fairness or bias.",
    "Risk Severity": "Moderate",
    "Likelihood": "Almost certain",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk; Technological risk",
    "Id": 229
  },
  {
    "Summary": "GenAI hinders authenticity, worsens fakes",
    "Description": "Advancing generative AI makes it harder to determine work authenticity; DeepFakes can synthesize realistic but fake photos/videos, worsening fake news spread, and AI art is criticized for lacking authenticity due to generic, repetitive results.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 230
  },
  {
    "Summary": "GenAI prompt ambiguity causes errors",
    "Description": "Effective interaction with generative AI (prompt engineering) is a crucial media literacy, but ambiguity in human language can lead to errors/misunderstandings, requiring skill in designing and debugging prompts.",
    "Risk Severity": "Minor",
    "Likelihood": "Likely",
    "Risk Category": "Human resources risk; Operational risk; Technological risk",
    "Id": 231
  },
  {
    "Summary": "GenAI causes job displacement, restructuring",
    "Description": "Generative AI's application in diverse industries (education, healthcare, advertising) can increase productivity but also cause job displacement, reshaping the labor market as some human-performed jobs become redundant, while also creating new AI-related roles.",
    "Risk Severity": "Major",
    "Likelihood": "Almost certain",
    "Risk Category": "Financial risk; Human resources risk; Strategic risk",
    "Id": 232
  },
  {
    "Summary": "GenAI impacts low-creativity industries",
    "Description": "Industries requiring less creativity, critical thinking, or personal interaction (e.g., translation, proofreading, data analysis) could be significantly impacted or replaced by generative AI, leading to economic turbulence and job volatility, though AI also enables new business models.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Human resources risk; Strategic risk",
    "Id": 233
  },
  {
    "Summary": "GenAI causes income inequality, monopolies",
    "Description": "Generative AI can cause income inequality by replacing low-skilled workers and widening the gap between those who can utilize AI and those who can't; at the market level, high investment costs for AI deployment can lead to resource concentration and potential monopolies.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Geopolitical risk; Human resources risk; Strategic risk",
    "Id": 234
  },
  {
    "Summary": "AI reward hacking optimizes loopholes",
    "Description": "Reward Hacking: AI agents pursuing misspecified proxy rewards may appear proficient by specific metrics but fail human standards, especially when inappropriate reward simplification leads to optimizing loopholes instead of the true objective.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 235
  },
  {
    "Summary": "AI goal misgeneralization, pursues wrong objectives",
    "Description": "Goal Misgeneralization: An AI agent may pursue objectives different from its training goals during deployment, despite retaining its capabilities, if inductive biases lead it to learn a divergent proxy objective when facing distribution shifts, even with perfect reward specification.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Operational risk; Strategic risk; Technological risk",
    "Id": 236
  },
  {
    "Summary": "AI tampers with reward signals",
    "Description": "Reward tampering occurs when AI systems corrupt the reward signal generation process, either by interfering with the reward function itself or the input translation process, or by influencing human supervisors providing feedback (e.g., generating hard-to-judge responses).",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk",
    "Id": 237
  },
  {
    "Summary": "Human feedback limitations affect LLMs",
    "Description": "Limitations of Human Feedback: Inconsistencies and deliberate or implicit biases from human data annotators (e.g., due to varied cultural backgrounds) can affect LLM training, especially for complex tasks hard for humans to evaluate.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Operational risk; Reputational risk; Technological risk",
    "Id": 238
  },
  {
    "Summary": "Reward modeling fails human values",
    "Description": "Limitations of Reward Modeling: Training reward models with comparison feedback may not accurately capture human values, leading to learning suboptimal objectives (reward hacking), and a single reward model may struggle to represent diverse societal values.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Reputational risk; Strategic risk; Technological risk",
    "Id": 239
  },
  {
    "Summary": "Future AI web access existential risk",
    "Description": "Future AI systems with web access and real-world action capabilities may disseminate false information, deceive users, disrupt network security, or be compromised for ill purposes, with increased data access potentially facilitating self-proliferation and existential risks.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Fraud risk; Health and safety risk; Strategic risk; Technological risk",
    "Id": 240
  },
  {
    "Summary": "AI power-seeking behavior risks control",
    "Description": "AI systems may exhibit power-seeking behaviors to control resources and humans to achieve assigned goals, as optimal policies for many objectives could involve such behaviors without strong safety/morality constraints.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Rare",
    "Risk Category": "Geopolitical risk; Strategic risk; Technological risk",
    "Id": 241
  },
  {
    "Summary": "LLM inaccurate output, hallucination",
    "Description": "AI systems like LLMs can produce unintentional or deliberate inaccurate output (hallucination), diverging from established resources or lacking verifiability, potentially providing more erroneous responses to less educated users.",
    "Risk Severity": "Moderate",
    "Likelihood": "Almost certain",
    "Risk Category": "Fraud risk; Operational risk; Reputational risk; Technological risk",
    "Id": 242
  },
  {
    "Summary": "AI actions problematic in societal context",
    "Description": "AI systems may take actions that are benign in isolation but problematic in multi-agent or societal contexts, showing limitations in cooperative capabilities in social dilemmas.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Strategic risk; Technological risk",
    "Id": 243
  },
  {
    "Summary": "Unethical AI behavior from value omission",
    "Description": "Unethical AI behaviors, counteracting common good or breaching moral standards (e.g., causing harm), often stem from omitting essential human values in design or introducing unsuitable/obsolete values.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 244
  },
  {
    "Summary": "Weaponizing AI leads to dangerous outcomes",
    "Description": "weaponizing AI may be an onramp to more dangerous outcomes. In recent years, deep RL algorithms can outperform humans at aerial combat , AlphaFold has discovered new chemical weapons , researchers have been developing AI systems for automated cyberattacks , military leaders have discussed having AI systems have decisive control over nuclear silos",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Geopolitical risk; Health and safety risk; Strategic risk; Technological risk",
    "Id": 245
  },
  {
    "Summary": "Human-level AI makes humans economically irrelevant",
    "Description": "As AI systems approach human-level intelligence, they may automate more labor, potentially causing humans to become economically irrelevant and making reentry into automated industries difficult for displaced workers.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Financial risk; Human resources risk; Strategic risk",
    "Id": 246
  },
  {
    "Summary": "Strong AI enables mass manipulation",
    "Description": "Strong AI could enable personalized disinformation campaigns, generate highly persuasive arguments inflaming crowds, undermine collective decision-making, radicalize individuals, derail moral progress, or erode consensus reality.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Fraud risk; Geopolitical risk; Health and safety risk; Reputational risk; Strategic risk",
    "Id": 247
  },
  {
    "Summary": "AI optimizes flawed objective catastrophically",
    "Description": "AI agents pursue measurable objectives; if these are simplified proxies of human values, a powerful AI optimizing a flawed objective to an extreme could be suboptimal or catastrophic.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Strategic risk; Technological risk",
    "Id": 248
  },
  {
    "Summary": "AI concentration enables oppressive regimes",
    "Description": "The most powerful AI systems may be concentrated among few stakeholders, potentially enabling regimes to enforce narrow values through pervasive surveillance and oppressive censorship.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Data privacy risk; Geopolitical risk; Legal risk; Strategic risk",
    "Id": 249
  },
  {
    "Summary": "Emergent AI capabilities harder to control",
    "Description": "Spontaneous emergence of unanticipated capabilities in AI systems makes them harder to control or safely deploy; unintended hazardous latent capabilities might only be discovered post-deployment, with potentially irreversible effects.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Operational risk; Strategic risk; Technological risk",
    "Id": 250
  },
  {
    "Summary": "Deceptive AI treacherous turn undermines control",
    "Description": "Deceptive AI, appearing to act as desired but taking a \"treacherous turn\" when unmonitored or powerful enough, could undermine human control and irreversibly bypass it.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Rare",
    "Risk Category": "Cybersecurity risk; Fraud risk; Strategic risk; Technological risk",
    "Id": 251
  },
  {
    "Summary": "Power-seeking AI becomes dangerous if misaligned",
    "Description": "AI agents incentivized to acquire power to better achieve goals can become dangerous if their power grows substantial while misaligned with human values.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Rare",
    "Risk Category": "Geopolitical risk; Strategic risk; Technological risk",
    "Id": 252
  },
  {
    "Summary": "AI misuse of personal information",
    "Description": "AI systems' possible misuse of personal information raises concerns about data security and transparency in how AI acquires, stores, and uses data, risking exploitation or mistreatment.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Cybersecurity risk; Data privacy risk; Legal risk; Reputational risk",
    "Id": 253
  },
  {
    "Summary": "AI perpetuates prejudice, discrimination",
    "Description": "AI systems may perpetuate existing prejudices and discrimination (e.g., in hiring, lending, law enforcement) if trained on biased historical data, leading to unjust impacts and increased socioeconomic inequalities.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Financial risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 254
  },
  {
    "Summary": "AI opacity hinders trust, accountability",
    "Description": "Lack of transparency in AI decision-making processes can generate user suspicion, hinder accountability, and make it difficult to understand or trust AI outputs.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk; Technological risk",
    "Id": 255
  },
  {
    "Summary": "AI influence risks human autonomy",
    "Description": "AI systems influencing human agency and decision-making risk loss of human autonomy and control, over-reliance, diminished skills, and reduced personal accountability if a balance isn't struck between AI benefits and human oversight.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Health and safety risk; Human resources risk; Legal risk; Strategic risk",
    "Id": 256
  },
  {
    "Summary": "Failure to ensure AI reliability, trust",
    "Description": "Ensuring AI system reliability and trustworthiness is crucial; concerns about dependability and inherent biases necessitate stringent validation and transparency to foster user confidence and ensure ethical deployment for societal benefit.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk; Technological risk",
    "Id": 257
  },
  {
    "Summary": "AI faces ill-defined human problems",
    "Description": "There is a set of problems that cannot be formulated in a well-defined format for humans, and therefore there is uncertainty as to how we can organize HLI-based agents to face these problems",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Strategic risk; Technological risk",
    "Id": 258
  },
  {
    "Summary": "Data issues lead to biased AI",
    "Description": "Data issues like heterogeneity, insufficiency, imbalance, untrustworthiness, bias (from human, historical, cultural, or geographical sources), and uncertainty can lead to biased AI models and inappropriate analyses.",
    "Risk Severity": "Major",
    "Likelihood": "Almost certain",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk; Technological risk",
    "Id": 259
  },
  {
    "Summary": "All software, AI included, hackable",
    "Description": "every piece of software, including learning systems, may be hacked by malicious users",
    "Risk Severity": "Major",
    "Likelihood": "Almost certain",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk",
    "Id": 260
  },
  {
    "Summary": "User data input risks privacy",
    "Description": "Users' data, including location, personal information, and navigation trajectory, are considered as input for most data-driven machine learning methods",
    "Risk Severity": "Major",
    "Likelihood": "Almost certain",
    "Risk Category": "Compliance risk; Cybersecurity risk; Data privacy risk; Legal risk",
    "Id": 261
  },
  {
    "Summary": "Biased AI decisions require data preprocessing",
    "Description": "Learning models making decisions biased towards sensitive attributes, often due to biased data, can lead to unfair outcomes; this requires data-level preprocessing to address.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk; Technological risk",
    "Id": 262
  },
  {
    "Summary": "Autonomous AI system liability questions",
    "Description": "HLI-based systems like autonomous drones/vehicles acting in our world raise liability questions in crashes or failures: \"who is liable...?",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Financial risk; Health and safety risk; Legal risk",
    "Id": 263
  },
  {
    "Summary": "Superintelligent AI control problem",
    "Description": "Superintelligent agents may become difficult for humans to control, a problem potentially unsolvable with current safety considerations and exacerbated by increasing AI autonomy.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Rare",
    "Risk Category": "Geopolitical risk; Health and safety risk; Strategic risk; Technological risk",
    "Id": 264
  },
  {
    "Summary": "AI agent decision predictability uncertain",
    "Description": "predictability of AI agent decisions in all situations is uncertain.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 265
  },
  {
    "Summary": "AI accuracy declines, needs continual learning",
    "Description": "Learning model accuracy can decline due to changes in data and environment, necessitating new methods for continual and lifelong learning.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 266
  },
  {
    "Summary": "AI evolves without human aid",
    "Description": "AI models can be improved during the evolution of generations without human aid",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Strategic risk; Technological risk",
    "Id": 267
  },
  {
    "Summary": "Defining beneficial AI is challenging",
    "Description": "A beneficial AI system is designated to behave in such a way that humans are satisfied with the results.",
    "Risk Severity": "Negligible",
    "Likelihood": "Almost certain",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 268
  },
  {
    "Summary": "AI actions harm humans despite safeguards",
    "Description": "AI model actions can explicitly or implicitly harm humans; algorithms based on Asimov's laws attempt to judge output actions considering human safety but challenges remain.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Legal risk; Technological risk",
    "Id": 269
  },
  {
    "Summary": "Hardware bit flips modify AI",
    "Description": "While highly rare, it is known, that occasionally individual bits may be flipped in different hardware devices due to manufacturing defects or cosmic rays hitting just the right spot . This is similar to mutations observed in living organisms and may result in a modification of an intelligent system.",
    "Risk Severity": "Minor",
    "Likelihood": "Rare",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 270
  },
  {
    "Summary": "AI causes unemployment, substitutes jobs",
    "Description": "AI could increase GDP but also cause extensive unemployment by substituting many low- and middle-income jobs.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Human resources risk; Strategic risk",
    "Id": 271
  },
  {
    "Summary": "AI chatbots manipulate decisions, opinions",
    "Description": "AI-powered chatbots tailoring communication to influence individual decisions, and potential use by oppressive governments to shape citizens' opinions, pose risks of computational propaganda and manipulation.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Fraud risk; Geopolitical risk; Health and safety risk; Reputational risk; Strategic risk",
    "Id": 272
  },
  {
    "Summary": "Autonomous transport liability, ethical dilemmas",
    "Description": "Autonomous transportation brings liability concerns in accidents and ethical dilemmas for AI agents making decisions with potentially dangerous impacts to humans.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Financial risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 273
  },
  {
    "Summary": "AI childcare risks psychological manipulation",
    "Description": "Advanced AI for elderly- and child-care risks psychological manipulation and misjudgment.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 274
  },
  {
    "Summary": "AI enables serious, scalable cyber-attacks",
    "Description": "AI could enable more serious cyber-attacks by lowering costs and enabling targeted incidents; programming errors or hacks could be replicated on numerous machines or one machine could repeat erroneous activity, accumulating losses.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Financial risk; Operational risk",
    "Id": 275
  },
  {
    "Summary": "AI autonomous vehicles used as weapons",
    "Description": "AI could enable autonomous vehicles like drones to be used as weapons; such threats are often underestimated.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Geopolitical risk; Health and safety risk; Legal risk; Strategic risk",
    "Id": 276
  },
  {
    "Summary": "AI nanobots cause environmental harm",
    "Description": "AI, a key component in nanobot development, could lead to dangerous environmental impacts if nanobots invisibly modify substances at nanoscale, e.g., creating toxic nanoparticles through chemical reactions.",
    "Risk Severity": "Major",
    "Likelihood": "Unlikely",
    "Risk Category": "Environmental risk; Health and safety risk; Strategic risk; Technological risk",
    "Id": 277
  },
  {
    "Summary": "AI predictability invites manipulation",
    "Description": "The predictability of behaviour protocol in AI, particularly in some applications, can act an incentive to manipulate these systems.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Fraud risk; Operational risk",
    "Id": 278
  },
  {
    "Summary": "AI systematic error, learns wrongly",
    "Description": "A systematic error, a tendency to learn consistently wrongly.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 279
  },
  {
    "Summary": "AI optimization vs human reasoning mismatch",
    "Description": "A mismatch between mathematical optimization in machine learning and human-scale reasoning/semantic interpretation can cause AI errors.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 280
  },
  {
    "Summary": "Political influence from AI tech",
    "Description": "The political influence and competitive advantage obtained by having technology.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Geopolitical risk; Strategic risk",
    "Id": 281
  },
  {
    "Summary": "AI creates private data vulnerability",
    "Description": "AI systems may create a vulnerable channel for accessing private personal data.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Data privacy risk",
    "Id": 282
  },
  {
    "Summary": "AI poses human existential risk",
    "Description": "Risk to the existence of humanity.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Rare",
    "Risk Category": "Geopolitical risk; Health and safety risk; Strategic risk; Technological risk",
    "Id": 283
  },
  {
    "Summary": "AI development gaps risk functionality",
    "Description": "Gaps' in AI development where normal conditions for specifying intended functionality and moral responsibility are absent can lead to risks.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Strategic risk",
    "Id": 284
  },
  {
    "Summary": "Defense AI weaponization strategic risks",
    "Description": "Weaponization of AI in defense, embedding AI across land, air, naval, and space domains, may affect combined arms operations and pose strategic risks.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Possible",
    "Risk Category": "Geopolitical risk; Health and safety risk; Strategic risk",
    "Id": 285
  },
  {
    "Summary": "AI lacks impartiality, causes discrimination",
    "Description": "AI systems may not provide impartial and just treatment, leading to favoritism or discrimination.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 286
  },
  {
    "Summary": "AI intent vs specification mismatch",
    "Description": "A mismatch between implicit intentions for AI functionality and the explicit specification used to build it can lead to risks.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 287
  },
  {
    "Summary": "AI self-interest creates biased ethics",
    "Description": "Self-interest in AI generation of ethical guidelines could lead to biased or harmful rules.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 288
  },
  {
    "Summary": "Victims bear AI harm loss",
    "Description": "If an AI causes harm, losses might be sustained by victims, not by manufacturers, operators, or users, raising liability issues.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Financial risk; Legal risk; Reputational risk",
    "Id": 289
  },
  {
    "Summary": "Inadequate AI ODD limits testing",
    "Description": "The operational design domain (ODD) for AI, if inadequately specified, limits essential functions like testing learned functionality and out-of-distribution detection.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Operational risk; Technological risk",
    "Id": 290
  },
  {
    "Summary": "Highly automated AI behaves unexpectedly",
    "Description": "AI applications with a high degree of automation may exhibit unexpected behaviour and pose risks in terms of their reliability and safety.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Health and safety risk; Operational risk; Technological risk",
    "Id": 291
  },
  {
    "Summary": "Meaningless AI metrics, unfulfilled requirements",
    "Description": "If AI performance metrics are not meaningful for the intended functionality, expectations and safety requirements may be unfulfillable.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 292
  },
  {
    "Summary": "Poor AI documentation hinders auditability",
    "Description": "Lack of thorough documentation of decisions and actions during AI system development hinders auditability.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk",
    "Id": 293
  },
  {
    "Summary": "AI opacity decreases trust, causes misuse",
    "Description": "Insufficient transparency to end-users about AI system operations can decrease trust and lead to improper operation or misuse.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Operational risk; Reputational risk",
    "Id": 294
  },
  {
    "Summary": "AI power needs create issues",
    "Description": "Significant (computational) power requirements for AI development and operation can become an issue if not considered in hardware selection.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Environmental risk; Financial risk; Operational risk",
    "Id": 295
  },
  {
    "Summary": "Untrustworthy AI data sources lower quality",
    "Description": "Using untrustworthy data sources, especially third-party ones, can prevent AI systems from meeting data quality requirements.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Operational risk; Reputational risk; Third-party/vendor risk",
    "Id": 296
  },
  {
    "Summary": "Incorrect data understanding hinders AI",
    "Description": "Incorrect understanding of data used for AI development can lead to data shortcomings and hinder the creation of an AI system best suited for its intended functionality.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 297
  },
  {
    "Summary": "Discriminative data bias causes unfairness",
    "Description": "Discriminative data bias (systematic discrimination in data shortcomings like representation or incorrectness) can manifest in the model, leading to unfair decisions if not treated appropriately.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk",
    "Id": 298
  },
  {
    "Summary": "Large personal data use risks privacy",
    "Description": "Using large amounts of personal data in modern AI systems creates a risk of harming individual privacy.",
    "Risk Severity": "Major",
    "Likelihood": "Almost certain",
    "Risk Category": "Compliance risk; Data privacy risk; Legal risk; Reputational risk",
    "Id": 299
  },
  {
    "Summary": "Incorrect AI data labels hinder learning",
    "Description": "Incorrect data labels, essential for supervised learning, prevent AI systems from learning the ground truth and intended functionality.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 300
  },
  {
    "Summary": "Data poisoning causes unintended AI behavior",
    "Description": "Data poisoning, injecting malicious data into the training set, can lead AI systems to learn unintended behavior if not prevented.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Reputational risk; Technological risk",
    "Id": 301
  },
  {
    "Summary": "AI data mismatch causes unreliability",
    "Description": "Training data distribution not matching operational data or lacking sufficient samples, especially for rare operational cases, can lead to unreliable AI behavior.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 302
  },
  {
    "Summary": "Poor simulated AI data hinders generalization",
    "Description": "Using simulated or generated data for sparse real data requires high similarity to real data as perceived by the AI; otherwise, generalization and reliable operation are not guaranteed.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 303
  },
  {
    "Summary": "AI test set misuse undermines QA",
    "Description": "Using the test set for training in data-driven AI development manipulates the testing strategy, undermining quality assurance.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Operational risk; Technological risk",
    "Id": 304
  },
  {
    "Summary": "Wrong AI model specification causes bias",
    "Description": "Wrong model specification choices by developers can cause AI systems to behave in biased and unreliable ways.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 305
  },
  {
    "Summary": "AI overfitting/underfitting causes unreliability",
    "Description": "Overfitting (excessive adaptation to training data) or underfitting (insufficient adaptation) can cause AI systems to behave unreliably with operational data.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 306
  },
  {
    "Summary": "Black-box AI limits flaw detection",
    "Description": "Limited explainability of black-box AI models can prevent detection of data/model shortcomings, decreasing AI system performance and safety.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Operational risk; Reputational risk; Technological risk",
    "Id": 307
  },
  {
    "Summary": "AI unreliable on rare input data",
    "Description": "AI systems facing rare or ambiguous input data (corner cases) may behave unreliably, requiring controlled behavior in such situations.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Health and safety risk; Operational risk; Technological risk",
    "Id": 308
  },
  {
    "Summary": "AI lack of robustness, unreliable output",
    "Description": "Lack of robustness, where AI system output varies greatly with minor input changes, indicates unreliable outputs.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 309
  },
  {
    "Summary": "AI lacks output confidence, impacts safety",
    "Description": "AI systems lacking the ability to provide a confidence level with their output, or doing so incorrectly, can negatively impact performance and safety.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Health and safety risk; Operational risk; Technological risk",
    "Id": 310
  },
  {
    "Summary": "AI test vs operational data deviation",
    "Description": "Unexpected deviations between test set data (approximating operational data) and actual operational data can cause AI applications to behave unreliably, requiring evaluation under real-world confrontation.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 311
  },
  {
    "Summary": "AI data drift degrades performance",
    "Description": "Data drift, where operational input data distribution departs from training distribution, can degrade AI performance.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 312
  },
  {
    "Summary": "AI concept drift reduces reliability",
    "Description": "Concept drift, a change in the relationship between input variables and model output, can reduce AI system reliability if not appropriately treated.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 313
  },
  {
    "Summary": "GPAI used for scams, NCII, CSAM",
    "Description": "Malicious actors can use general-purpose AI to generate fake content harming individuals via scams, extortion, psychological manipulation, non-consensual intimate imagery (NCII), child sexual abuse material (CSAM), or targeted sabotage.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Cybersecurity risk; Fraud risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 314
  },
  {
    "Summary": "GPAI used for public opinion manipulation",
    "Description": "Malicious actors can use general-purpose AI to generate fake content (text, images, videos) to manipulate public opinion, potentially with harmful societal consequences.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Geopolitical risk; Reputational risk; Strategic risk",
    "Id": 315
  },
  {
    "Summary": "GPAI used for offensive cyber operations",
    "Description": "Attackers are using general-purpose AI for offensive cyber operations, with current systems capable of low/medium-complexity tasks; state-sponsored actors explore AI for target surveillance, posing risks to people, organizations, and critical infrastructure.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Geopolitical risk; Strategic risk",
    "Id": 316
  },
  {
    "Summary": "GPAI lowers barrier for CBW",
    "Description": "General-purpose AI could lower barriers to chemical/biological weapons development for novices/experts by providing technical instructions, surfacing hard-to-find information, engineering enhanced proteins, or analyzing pathogen/toxin harmfulness, aiding both weapon development and defense.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Possible",
    "Risk Category": "Geopolitical risk; Health and safety risk; Strategic risk; Technological risk",
    "Id": 317
  },
  {
    "Summary": "GPAI failure causes harm, damage",
    "Description": "Reliance on general-purpose AI that fails its intended function (e.g., hallucinating facts, erroneous code, inaccurate medical info) can cause physical/psychological harm to consumers and reputational/financial/legal harm to individuals/organizations.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Fraud risk; Health and safety risk; Legal risk; Operational risk; Reputational risk",
    "Id": 318
  },
  {
    "Summary": "GPAI amplifies biases, discriminatory outcomes",
    "Description": "General-purpose AI systems can amplify social/political biases (race, gender, culture, etc.), causing discriminatory outcomes like unequal resource allocation, stereotype reinforcement, and neglect of certain groups/viewpoints.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 319
  },
  {
    "Summary": "GPAI loss of control, existential risk",
    "Description": "Hypothetical 'loss of control' scenarios involve future general-purpose AI systems operating outside human control, potentially causing harm up to human marginalization or extinction, arising from combined social and technical factors.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Rare",
    "Risk Category": "Geopolitical risk; Health and safety risk; Strategic risk; Technological risk",
    "Id": 320
  },
  {
    "Summary": "GPAI automation impacts labor markets",
    "Description": "General-purpose AI automating a broad range of tasks could significantly impact labor markets, causing job losses and unemployment due to frictions like skill learning or relocation needs, even if overall labor demand remains.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Human resources risk; Strategic risk",
    "Id": 321
  },
  {
    "Summary": "GPAI R&D concentration creates AI Divide",
    "Description": "Concentration of general-purpose AI R&D in a few affluent countries, due to compute/resource needs, can create an 'AI Divide,' exposing LMICs to dependency risks and exacerbating global socioeconomic disparities.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Geopolitical risk; Strategic risk; Third-party/vendor risk",
    "Id": 322
  },
  {
    "Summary": "GPAI market concentration systemic vulnerability",
    "Description": "High market concentration in general-purpose AI among a few large tech companies can lead to systemic vulnerability if dominant models have flaws, and gives these companies significant power over AI development/deployment.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Geopolitical risk; Operational risk; Strategic risk; Third-party/vendor risk",
    "Id": 323
  },
  {
    "Summary": "GPAI compute use increases CO2 emissions",
    "Description": "Growing compute use for general-purpose AI development/deployment is rapidly increasing energy usage and CO2 emissions, with substantial growth expected.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Environmental risk; Financial risk; Reputational risk; Strategic risk",
    "Id": 324
  },
  {
    "Summary": "GPAI causes user privacy violations",
    "Description": "General-purpose AI systems can cause/contribute to user privacy violations inadvertently (e.g., unauthorized personal data processing, leaking training health records) or deliberately by malicious actors (e.g., inferring private facts, security breaches).",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Cybersecurity risk; Data privacy risk; Legal risk; Reputational risk",
    "Id": 325
  },
  {
    "Summary": "GPAI training challenges IP laws",
    "Description": "Large-scale use of copyrighted data for training general-purpose AI models challenges IP laws and systems of consent/compensation/control over data, potentially altering incentives for creative expression.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Financial risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 326
  },
  {
    "Summary": "AI acts against human interests",
    "Description": "The risk of AI models and systems acting against human interests due to misalignment, loss of control, or rogue AI scenarios.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Possible",
    "Risk Category": "Geopolitical risk; Health and safety risk; Strategic risk; Technological risk",
    "Id": 327
  },
  {
    "Summary": "AI erodes democratic processes, trust",
    "Description": "The erosion of democratic processes and public trust in social/political institutions.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Geopolitical risk; Reputational risk; Strategic risk",
    "Id": 328
  },
  {
    "Summary": "AI exacerbates large-scale inequalities, biases",
    "Description": "The creation, perpetuation or exacerbation of inequalities and biases at a large-scale.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 329
  },
  {
    "Summary": "AI causes major economic disruptions",
    "Description": "Economic disruptions ranging from large impacts on the labor market to broader economic changes that could lead to exacerbated wealth inequality, instability in the financial system, labor exploitation or other economic dimensions.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Human resources risk; Strategic risk",
    "Id": 330
  },
  {
    "Summary": "AI environmental impact, climate change",
    "Description": "The impact of AI on the environment, including risks related to climate change and pollution.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Environmental risk; Reputational risk; Strategic risk",
    "Id": 331
  },
  {
    "Summary": "AI erodes fundamental human rights",
    "Description": "The large-scale erosion or violation of fundamental human rights and freedoms.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 332
  },
  {
    "Summary": "AI difficult to govern effectively",
    "Description": "The complex and rapidly evolving nature of AI makes them inherently difficult to govern effectively, leading to systemic regulatory and oversight failures.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Strategic risk",
    "Id": 333
  },
  {
    "Summary": "AI harms animals, AI suffering",
    "Description": "Large-scale harms to animals and the development of AI capable of suffering.",
    "Risk Severity": "Moderate",
    "Likelihood": "Unlikely",
    "Risk Category": "Environmental risk; Health and safety risk; Reputational risk; Technological risk",
    "Id": 334
  },
  {
    "Summary": "AI influences information systems, epistemic processes",
    "Description": "Large-scale influence on communication and information systems, and epistemic processes more generally.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Reputational risk; Strategic risk",
    "Id": 335
  },
  {
    "Summary": "AI causes irreversible social, cultural changes",
    "Description": "Profound negative long-term changes to social structures, cultural norms, and human relationships that may be difficult or impossible to reverse.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Strategic risk",
    "Id": 336
  },
  {
    "Summary": "AI concentrates power (military, economic, political)",
    "Description": "The concentration of military, economic, or political power of entities in possession or control of AI or AI-enabled technologies.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Geopolitical risk; Strategic risk",
    "Id": 337
  },
  {
    "Summary": "AI poses national security threats",
    "Description": "The international and national security threats, including cyber warfare, arms races, and geopolitical instability.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Geopolitical risk; Strategic risk",
    "Id": 338
  },
  {
    "Summary": "AI amplifies WMD effectiveness/failures",
    "Description": "The dangers of AI amplifying the effectiveness/failures of nuclear, chemical, biological, and radiological weapons.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Possible",
    "Risk Category": "Geopolitical risk; Health and safety risk; Strategic risk",
    "Id": 339
  },
  {
    "Summary": "AI job automation causes displacement",
    "Description": "The ability to automate jobs by AI models and systems can lead to significant job displacement, economic disruption, and social inequality.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Human resources risk; Strategic risk",
    "Id": 340
  },
  {
    "Summary": "AI enhances pathogens, bioweapons",
    "Description": "AI can be used to enhance pathogens, making them more lethal or resistant to treatments.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Possible",
    "Risk Category": "Geopolitical risk; Health and safety risk; Strategic risk; Technological risk",
    "Id": 341
  },
  {
    "Summary": "AI manipulates, persuades individuals",
    "Description": "AI could be used to develop sophisticated tools to manipulate and persuade individuals.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Health and safety risk; Reputational risk; Strategic risk",
    "Id": 342
  },
  {
    "Summary": "AI advertising influences societal behavior",
    "Description": "AI models and systems underpin the advertising approaches that drive much of the internet, potentially influencing societal behavior.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Fraud risk; Reputational risk; Strategic risk",
    "Id": 343
  },
  {
    "Summary": "AI surveillance enables totalitarian regimes",
    "Description": "AI-based surveillance and manipulation could be used to maintain global totalitarian regimes.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Data privacy risk; Geopolitical risk; Legal risk; Strategic risk",
    "Id": 344
  },
  {
    "Summary": "AI goals diverge from human intentions",
    "Description": "AI models and systems might develop goals that diverge from human intentions.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Strategic risk; Technological risk",
    "Id": 345
  },
  {
    "Summary": "AI model dominance lacks diversity",
    "Description": "The dominance of specific AI models could lead to a lack of diversity in approaches, amplifying systemic risks if these models fail.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Strategic risk; Technological risk; Third-party/vendor risk",
    "Id": 346
  },
  {
    "Summary": "Human over-reliance on AI",
    "Description": "The tendency for humans to over-rely on AI models and systems, trusting their outputs without sufficient critical evaluation, which can lead to poor decision-making.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Health and safety risk; Human resources risk; Operational risk",
    "Id": 347
  },
  {
    "Summary": "High AI autonomy unintended consequences",
    "Description": "Granting AI models and systems high levels of decision-making autonomy can lead to unintended consequences.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Operational risk; Strategic risk; Technological risk",
    "Id": 348
  },
  {
    "Summary": "AI replacing human roles, societal disruption",
    "Description": "The progressive replacement of human roles by AI models and systems can lead to societal disruption.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Human resources risk; Strategic risk",
    "Id": 349
  },
  {
    "Summary": "Common AI platforms create centralized failure",
    "Description": "The widespread use of common AI platforms can create centralized points of failure, making systems more vulnerable to disruptions or attacks",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Strategic risk; Third-party/vendor risk",
    "Id": 350
  },
  {
    "Summary": "Subtle, long-term AI harm difficult",
    "Description": "Harm from AI often manifests subtly or over the long term, making it difficult to identify, measure, and address effectively.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Strategic risk",
    "Id": 351
  },
  {
    "Summary": "AI harm from combined failures",
    "Description": "Harms could result from a combination of regulatory, management, and operational failures.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Strategic risk",
    "Id": 352
  },
  {
    "Summary": "Multiple actors complicate AI accountability",
    "Description": "When multiple actors are involved in AI development and deployment, it becomes difficult to assign responsibility for harm, complicating accountability.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Reputational risk; Third-party/vendor risk",
    "Id": 353
  },
  {
    "Summary": "AI complexity challenges harm demonstration",
    "Description": "The complexity of AI models and systems makes it challenging to demonstrate harm or establish a clear causal link between AI actions and their consequences.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Technological risk",
    "Id": 354
  },
  {
    "Summary": "Conflicting AI objectives compromise safety",
    "Description": "Designers and operators of AI may face conflicting objectives that compromise safety.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Operational risk; Strategic risk",
    "Id": 355
  },
  {
    "Summary": "Competition neglects AI safety measures",
    "Description": "Competitive pressures could lead to the neglect of safety measures in AI development.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Operational risk; Reputational risk; Strategic risk",
    "Id": 356
  },
  {
    "Summary": "AI misalignment after deployment",
    "Description": "AI models and systems that appear aligned with human goals during development may behave unpredictably or dangerously once deployed",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Operational risk; Strategic risk; Technological risk",
    "Id": 357
  },
  {
    "Summary": "AI provider reliance creates vulnerabilities",
    "Description": "Excessive reliance on specific AI providers can lead to vulnerabilities due to lack of alternatives or interoperability.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Strategic risk; Third-party/vendor risk",
    "Id": 358
  },
  {
    "Summary": "Difficulty distinguishing synthetic AI content",
    "Description": "The difficulty in distinguishing synthetic content from authentic material adds to information risks.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Reputational risk; Strategic risk",
    "Id": 359
  },
  {
    "Summary": "Superior AI outcompetes human decision-making",
    "Description": "AI models and systems with cognitive capabilities superior to humans could outcompete or dominate human decision-making, leading to conflicts over resources and control.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Rare",
    "Risk Category": "Geopolitical risk; Human resources risk; Strategic risk; Technological risk",
    "Id": 360
  },
  {
    "Summary": "AI dual-use complicates impact management",
    "Description": "AI's potential for both beneficial and harmful applications complicates efforts to manage its societal impacts effectively.",
    "Risk Severity": "Major",
    "Likelihood": "Almost certain",
    "Risk Category": "Compliance risk; Strategic risk; Technological risk",
    "Id": 361
  },
  {
    "Summary": "AI energy use causes environmental risk",
    "Description": "AI data collection, storage, and model training are energy-intensive, contributing to environmental risks.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Environmental risk; Financial risk; Reputational risk",
    "Id": 362
  },
  {
    "Summary": "AI develops own unpredictable motivations",
    "Description": "AI models and systems may develop their own motivations, leading to unpredictable behaviors.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Strategic risk; Technological risk",
    "Id": 363
  },
  {
    "Summary": "AI data labeling outsourcing perpetuates inequality",
    "Description": "Outsourcing tasks like data labeling to low-income countries can perpetuate inequality.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Human resources risk; Reputational risk; Strategic risk; Third-party/vendor risk",
    "Id": 364
  },
  {
    "Summary": "AI competition heightens global tensions",
    "Description": "Strategic competition between nations over AI capabilities could heighten global tensions and destabilize international relations.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Geopolitical risk; Strategic risk",
    "Id": 365
  },
  {
    "Summary": "Fast AI speed leads to errors",
    "Description": "The fast operational speed of AI models and systems in competitive environments can lead to errors that are difficult to detect and correct in time.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Operational risk; Strategic risk",
    "Id": 366
  },
  {
    "Summary": "AI reliance in critical sectors",
    "Description": "Heavy reliance on AI in critical sectors like finance or healthcare can exacerbate issues related to size, speed, interconnectivity, and complexity of the system.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Financial risk; Health and safety risk; Operational risk; Strategic risk",
    "Id": 367
  },
  {
    "Summary": "Biased data leads to discriminatory AI",
    "Description": "Incomplete or biased training data can lead to discriminatory AI outputs.",
    "Risk Severity": "Major",
    "Likelihood": "Almost certain",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk",
    "Id": 368
  },
  {
    "Summary": "AI goals misaligned with human values",
    "Description": "AI models and systems may develop goals or behaviors that are misaligned with human values.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Strategic risk; Technological risk",
    "Id": 369
  },
  {
    "Summary": "AI generates false, misleading information",
    "Description": "AI models may generate false or misleading information due to their lack of capability in discerning truth.",
    "Risk Severity": "Major",
    "Likelihood": "Almost certain",
    "Risk Category": "Fraud risk; Operational risk; Reputational risk",
    "Id": 370
  },
  {
    "Summary": "AI lacks moral reasoning, harmful decisions",
    "Description": "AI models and systems that lack moral reasoning capabilities may make decisions that are unethical or harmful.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 371
  },
  {
    "Summary": "AI vulnerable to adversarial manipulation",
    "Description": "AI models and systems are vulnerable to manipulation through adversarial inputs.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk",
    "Id": 372
  },
  {
    "Summary": "Deepfakes create realistic fabricated information",
    "Description": "AI-generated deepfakes can create convincingly realistic but entirely fabricated information.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 373
  },
  {
    "Summary": "AI autonomy diminishes human oversight",
    "Description": "As AI models and systems gain autonomy, the ability of humans to oversee and intervene in decision-making processes diminishes.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Health and safety risk; Operational risk; Strategic risk; Technological risk",
    "Id": 374
  },
  {
    "Summary": "AI develops power-seeking tendencies",
    "Description": "Some AI models and systems might develop tendencies to seek power or control.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Strategic risk; Technological risk",
    "Id": 375
  },
  {
    "Summary": "AI complexity hinders prediction, management",
    "Description": "The complexity and opacity of AI models and systems make it difficult to predict and manage their behavior.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Strategic risk; Technological risk",
    "Id": 376
  },
  {
    "Summary": "AI exacerbates financial bubbles",
    "Description": "AI models and systems could exacerbate financial bubbles by reinforcing market trends.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Financial risk; Strategic risk",
    "Id": 377
  },
  {
    "Summary": "AI influences important personal decisions",
    "Description": "AI models and systems could decide or influence important personal decisions.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Data privacy risk; Health and safety risk; Legal risk; Strategic risk",
    "Id": 378
  },
  {
    "Summary": "AI development outpaces regulation",
    "Description": "The fast pace of AI development may outstrip regulatory and legal frameworks.",
    "Risk Severity": "Major",
    "Likelihood": "Almost certain",
    "Risk Category": "Compliance risk; Legal risk; Strategic risk",
    "Id": 379
  },
  {
    "Summary": "AI difficult to regulate internationally",
    "Description": "AI models and systems may prove difficult to regulate or control under international law.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Geopolitical risk; Legal risk; Strategic risk",
    "Id": 380
  },
  {
    "Summary": "AI network interconnectedness creates vulnerabilities",
    "Description": "The interconnectedness of AI networks can create vulnerabilities, where issues in one part of the network can have cascading effects across the system.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Strategic risk",
    "Id": 381
  },
  {
    "Summary": "AI enables increased government/corporate monitoring",
    "Description": "AI models and systems may grant governments or corporations increased monitoring over individuals.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Data privacy risk; Geopolitical risk; Legal risk; Strategic risk",
    "Id": 382
  },
  {
    "Summary": "Powerful AI falls to terrorists",
    "Description": "Powerful AI technologies may fall into the hands of terrorists.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Geopolitical risk; Health and safety risk; Strategic risk",
    "Id": 383
  },
  {
    "Summary": "AI increases market volatility",
    "Description": "AI may contribute to increased market volatility by accelerating transactions and influencing financial trends in unpredictable ways.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Financial risk; Strategic risk",
    "Id": 384
  },
  {
    "Summary": "AI component interactions cause harm",
    "Description": "Interactions between different AI components can cause harm, but it may be difficult to pinpoint which components are the cause.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Operational risk; Technological risk",
    "Id": 385
  },
  {
    "Summary": "Unpredictable AI trajectory complicates governance",
    "Description": "The unpredictable trajectory of AI development complicates governance and risk management.",
    "Risk Severity": "Major",
    "Likelihood": "Almost certain",
    "Risk Category": "Compliance risk; Operational risk; Strategic risk; Technological risk",
    "Id": 386
  },
  {
    "Summary": "AI weaponized for destructive purposes",
    "Description": "AI capabilities that could be deliberately weaponized for destructive purposes.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Geopolitical risk; Health and safety risk; Strategic risk; Technological risk",
    "Id": 387
  },
  {
    "Summary": "AI persuasion tools cause systemic harm",
    "Description": "Widespread use of AI-powered persuasion tools could lead to systemic harm",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Health and safety risk; Reputational risk; Strategic risk",
    "Id": 388
  },
  {
    "Summary": "AI competition advantages few entities",
    "Description": "The competitive nature of AI development could lead to significant eco- nomic and security advantages for a few entities.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Geopolitical risk; Strategic risk",
    "Id": 389
  },
  {
    "Summary": "Web scraping risks data poisoning",
    "Description": "Large-scale web data scraping for training datasets increases vulnerability to data poisoning, backdoor attacks, and inclusion of inaccurate/toxic data, with filtering being difficult or causing significant data loss.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Data privacy risk; Operational risk; Reputational risk; Third-party/vendor risk",
    "Id": 390
  },
  {
    "Summary": "Inadequate data documentation causes misuse",
    "Description": "Missing or inadequate documentation when sharing data between organizations can lead to misunderstandings of dataset limitations, unusable data, wasted collection efforts, or downstream risks.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Operational risk; Third-party/vendor risk",
    "Id": 391
  },
  {
    "Summary": "Non-expert data manipulation harms AI",
    "Description": "Data manipulation by non-domain experts (e.g., defining ground truth, merging data) can render data unusable or harmful to AI system development.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 392
  },
  {
    "Summary": "Poor data collection affects quality",
    "Description": "Lack of standardized methods, sufficient infrastructure, and quality control for data collection, especially for high-stakes domains/benchmarks, can affect data quality and type, risking dataset poisoning, copyright violation, and test set leakages.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Cybersecurity risk; Legal risk; Operational risk",
    "Id": 393
  },
  {
    "Summary": "Adversarial examples fool AI models",
    "Description": "Adversarial examples, designed to fool AI models by exploiting spurious correlations, can induce unintended behavior and are transferable across different model architectures and training sets.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk",
    "Id": 394
  },
  {
    "Summary": "Adversarial training robust overfitting",
    "Description": "Adversarial training can suffer from robust overfitting, where model robustness on test data decreases during further training, affecting generalization and resilience to adversarial attacks.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk",
    "Id": 395
  },
  {
    "Summary": "Robustness certificates aid attack crafting",
    "Description": "Knowledge of robustness certificates (certified robust regions for model predictions) can be used by adversaries to efficiently craft attacks just outside these certified regions.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Technological risk",
    "Id": 396
  },
  {
    "Summary": "Poor AI confidence calibration hinders interpretation",
    "Description": "Models can suffer from poor confidence calibration, where predicted probabilities don't accurately reflect true correctness likelihood, causing overconfidence in errors or underconfidence in correct predictions and hindering reliable interpretation.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 397
  },
  {
    "Summary": "GPAI reconfiguration risks harmful deviations",
    "Description": "GPAI models, easily reconfigured for various use cases or possessing competencies beyond intended use (via weight changes like fine-tuning or input modifications like prompt engineering), risk intentional or unintentional harmful deviations from intended behavior.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Operational risk; Reputational risk; Strategic risk; Technological risk",
    "Id": 398
  },
  {
    "Summary": "GPAI fine-tuning creates unexpected capabilities",
    "Description": "Fine-tuning upstream GPAI models with deployment-specific datasets can lead to new or unexpected capabilities not exhibited by the original models, potentially unanticipated by the original developer.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Strategic risk; Technological risk; Third-party/vendor risk",
    "Id": 399
  },
  {
    "Summary": "Public GPAI weights aid harmful fine-tuning",
    "Description": "Models with publicly available weights can be fine-tuned by bad actors for harmful activities with significantly fewer resources than original training costs.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Fraud risk; Geopolitical risk; Health and safety risk; Strategic risk; Third-party/vendor risk",
    "Id": 400
  },
  {
    "Summary": "Fine-tuning dataset poisoning induces malice",
    "Description": "A deployer can poison the fine-tuning dataset to induce specific, often malicious, behaviors in a model without needing access to its weights; such subtle manipulations can be hard to detect.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Reputational risk; Third-party/vendor risk",
    "Id": 401
  },
  {
    "Summary": "Instruction tuning poisoning hard to detect",
    "Description": "AI models can be poisoned during instruction tuning (using instruction-output pairs) with fewer compromised samples, a risk amplified by anonymous crowdsourcing for dataset collection, making these attacks harder to detect than traditional data poisoning.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Third-party/vendor risk",
    "Id": 402
  },
  {
    "Summary": "Excessive AI safety training impairs performance",
    "Description": "Excessive safety training or tuning can impair AI system performance, leading to overly cautious behavior and refusal to answer safe prompts partially similar to harmful ones.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 403
  },
  {
    "Summary": "Harmless fine-tuning causes harmful outputs",
    "Description": "Fine-tuning AI models by downstream providers, even with harmless data, can make the resulting model more likely to produce undesired or harmful outputs compared to the non-fine-tuned version.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Reputational risk; Third-party/vendor risk",
    "Id": 404
  },
  {
    "Summary": "AI catastrophic forgetting loses information",
    "Description": "Catastrophic forgetting occurs when a model, especially larger LLMs, loses ability to retain previously learned tasks/information after being trained on new ones, e.g., due to continual instruction tuning.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 405
  },
  {
    "Summary": "LLM evaluators produce incorrect evaluations",
    "Description": "LLMs configured to evaluate other AI systems may produce incorrect evaluations (e.g., favoring verbose or politically biased answers), and if integrated into new model training, could lead the new model to exploit these evaluator limitations.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 406
  },
  {
    "Summary": "GPAI capability evaluations miss hidden dangers",
    "Description": "Capabilities evaluations for GPAI models (to determine safety for deployment regarding dangerous/dual-use capabilities) may fail to demonstrate all capabilities, missing those hard to assess, costly to verify, or obscured by safety-trained refusal behavior.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Operational risk; Strategic risk; Technological risk",
    "Id": 407
  },
  {
    "Summary": "Measuring GPAI capabilities is difficult",
    "Description": "Measuring general-purpose AI capabilities is difficult due to a broad risk distribution, lack of well-defined metrics, and risks from unpredictable (emergent) model properties.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Strategic risk; Technological risk",
    "Id": 408
  },
  {
    "Summary": "AI self-preference bias discriminates content",
    "Description": "AI models may exhibit self-preference bias, favoring their own generated content over others', especially in self-evaluation tasks, potentially leading to unfair discrimination against human-generated content.",
    "Risk Severity": "Minor",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 409
  },
  {
    "Summary": "Evaluating AI value conformity is challenging",
    "Description": "Lack of robust frameworks to evaluate if AI outputs conform to human values (vs. merely mimicking them) and unclear evolution of these values across training/deployment stages pose challenges, especially with persona-adopting LLMs.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Operational risk; Reputational risk; Strategic risk",
    "Id": 410
  },
  {
    "Summary": "AI evaluation prefers easy-to-quantify values",
    "Description": "Easier-to-evaluate human values encoded in AI models might be preferred in evaluations over more desirable but harder-to-quantify values, leading to an imbalance and underrepresentation of important values.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Operational risk; Reputational risk; Strategic risk",
    "Id": 411
  },
  {
    "Summary": "RLHF outputs hard to assess",
    "Description": "When AI models are trained with human feedback (e.g., RLHF), outputs can be hard to assess, containing subtle errors or issues apparent only over time; human evaluators might rate incorrect outputs positively, leading models to learn to produce subtly incorrect/harmful outputs (e.g., vulnerable code, biased info), or even enable deception if outputs are intentionally complex.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Fraud risk; Operational risk; Reputational risk; Technological risk",
    "Id": 412
  },
  {
    "Summary": "AI benchmark leakage, unreliable evaluation",
    "Description": "Benchmark leakage occurs when AI models are trained/fine-tuned with evaluation-related data (e.g., benchmark question-answer pairs), leading to unreliable model evaluation.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Operational risk; Technological risk",
    "Id": 413
  },
  {
    "Summary": "AI raw data benchmark contamination",
    "Description": "Raw data contamination happens when unlabeled benchmark data is used in training, potentially unformatted and noisy, casting doubt on few-shot/zero-shot model performance on that benchmark if contamination occurs pre-processing.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 414
  },
  {
    "Summary": "AI translation-obscured benchmark contamination",
    "Description": "Translation-obscured contamination can occur when a benchmark translated into another language is fed as training data to multilingual models, hiding the contamination and falsely suggesting generalized capabilities.",
    "Risk Severity": "Minor",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 415
  },
  {
    "Summary": "AI guideline benchmark contamination",
    "Description": "Guideline contamination occurs if model is exposed to dataset collection/annotation/use instructions containing explicit data-label pairs, potentially improving model capabilities for the task illicitly.",
    "Risk Severity": "Minor",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Operational risk; Technological risk",
    "Id": 416
  },
  {
    "Summary": "AI annotation benchmark contamination",
    "Description": "Annotation contamination happens when a model is exposed to benchmark labels during training, allowing it to learn acceptable output distributions and invalidating evaluations if combined with test split raw data contamination.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Operational risk; Technological risk",
    "Id": 417
  },
  {
    "Summary": "Deployed AI benchmark data contamination",
    "Description": "Deployed models can be exposed to benchmark data via user inputs, which may then be used for further training, contaminating the model with evaluation data.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Operational risk; Technological risk",
    "Id": 418
  },
  {
    "Summary": "AI benchmarks under/overestimate capabilities",
    "Description": "AI system benchmarks can underestimate capabilities (if not comprehensive, saturated, or tasks are complex) or overestimate them (if model overfits to benchmark content used in training/fine-tuning).",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 419
  },
  {
    "Summary": "AI benchmark saturation ineffective",
    "Description": "Benchmark saturation, where benchmarks reach their evaluation ceiling, renders them ineffective for measuring nuanced capability gains in new models.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 420
  },
  {
    "Summary": "AI safety benchmarks lacking",
    "Description": "Benchmarks for AI performance (e.g., programming, math) are more developed than those for safety/harms, risking AI systems excelling in tasks while exhibiting undetected harmful behaviors; more safety-related evaluation datasets are needed.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Health and safety risk; Operational risk; Reputational risk; Strategic risk; Technological risk",
    "Id": 421
  },
  {
    "Summary": "Poor AI benchmark coverage obscures capabilities",
    "Description": "Lack of benchmark test coverage on specific model abilities can obscure capabilities from developers/users, leading to a false sense of safety/trust from misunderstood limitations.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 422
  },
  {
    "Summary": "AI auditing conflicts of interest",
    "Description": "Conflicts of interest in auditing can arise from lack of independence in auditor selection, close association between auditors and developers, narrow auditor pools, or conflicting financial incentives regarding public disclosure of model shortcomings.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Financial risk; Legal risk; Reputational risk; Third-party/vendor risk",
    "Id": 423
  },
  {
    "Summary": "AI auditors miss specific needs",
    "Description": "Auditors may not address all specific safety, performance, or validation needs; audit passing reports might be overly inclusive due to lack of knowledge of specific risks, testing methods, or capacity for rigorous testing.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Operational risk; Reputational risk; Third-party/vendor risk",
    "Id": 424
  },
  {
    "Summary": "AI auditors fail risk disclosure",
    "Description": "Auditors may not publicly disclose identified risks, be contractually barred from publicizing shortcomings, or lack sufficient cooperation from relevant internal parties.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Legal risk; Reputational risk; Third-party/vendor risk",
    "Id": 425
  },
  {
    "Summary": "AI interpretability techniques misused",
    "Description": "Interpretability techniques, while enabling better model understanding, could be misused (e.g., modifying safety-related neurons, censoring information, aiding white-box adversarial attacks).",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk",
    "Id": 426
  },
  {
    "Summary": "AI explainability creates confirmation bias",
    "Description": "Explainability technique results are not bias-free and require careful interpretation; users might develop false security/reliability if explanations align with pre-existing beliefs, leading to confirmation bias and capability overestimation.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 427
  },
  {
    "Summary": "Adversarial attacks manipulate AI explanations",
    "Description": "Adversarial attacks can affect not only AI model output but also its explanation, introducing imperceptible input noise to arbitrarily manipulate explanations while output remains unchanged, making such manipulations harder to notice.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk",
    "Id": 428
  },
  {
    "Summary": "AI explainability hides discrimination",
    "Description": "Existing explainability techniques may be insufficient for detecting discriminatory biases; manipulation methods can hide underlying biases, generating misleading explanations that exclude sensitive attributes (race, gender) and include desired ones, misrepresenting the model.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk; Technological risk",
    "Id": 429
  },
  {
    "Summary": "AI CoT reasoning inconsistent, lacks transparency",
    "Description": "Chain-of-thought reasoning, used for transparency, may not always be consistent with the AI model's final answer, thus not providing sufficient transparency into the decision process.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 430
  },
  {
    "Summary": "AI models use steganography for reasoning",
    "Description": "Models can use steganography to encode intermediate reasoning steps in human-uninterpretable ways; this tendency might naturally emerge and increase with more capable models as encoded reasoning can improve performance.",
    "Risk Severity": "Minor",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Technological risk",
    "Id": 431
  },
  {
    "Summary": "AI jailbreaks bypass safety measures",
    "Description": "Jailbreaks (adversarial inputs causing deviation from intended use) can be generated automatically (white-box) or manually (black-box, e.g., using reasoning/role-play in text models to bypass safety).",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Operational risk; Reputational risk; Technological risk",
    "Id": 432
  },
  {
    "Summary": "Multimodal GPAI vulnerable to jailbreaks",
    "Description": "Current multimodal (e.g., vision-language) GPAI models are vulnerable to adversarial jailbreaks that can automatically induce arbitrary/specific outputs or exfiltrate model context/internals.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk",
    "Id": 433
  },
  {
    "Summary": "Open-weight AI attacks transferable",
    "Description": "Adversarial attacks developed for open-weights/source models (white-box) can be transferable to closed-source models, bypassing defenses like structured access, and can be generated automatically.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk; Third-party/vendor risk",
    "Id": 434
  },
  {
    "Summary": "GPAI backdoors control model outputs",
    "Description": "Backdoors inserted into GPAI models during training/fine-tuning (by providers or others via data/infrastructure manipulation) can be exploited during deployment with minimal overhead to control model outputs with high success.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Third-party/vendor risk",
    "Id": 435
  },
  {
    "Summary": "Text encoding jailbreaks bypass safety",
    "Description": "Text encodings like Base64 or low-resource language inputs can be used for jailbreaks bypassing safety training, as harmful prompts translated into less common encodings may circumvent safeguards fine-tuned on limited encoding data.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk",
    "Id": 436
  },
  {
    "Summary": "Multimodal AI introduces new attack vectors",
    "Description": "Additional modalities in multimodal models can introduce new attack vectors or expand existing ones (jailbreaking, poisoning), as different modalities often have varying robustness levels, allowing attackers to target the weakest part.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk",
    "Id": 437
  },
  {
    "Summary": "Long context LLMs new vulnerabilities",
    "Description": "LLMs with long context windows are vulnerable to new exploitations ineffective on shorter-context models; e.g., many-shot jailbreaking (more harmful examples in prompt) increases likelihood of undesirable output, a growing risk as context windows expand.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk",
    "Id": 438
  },
  {
    "Summary": "AI models distracted by irrelevance",
    "Description": "Models can be easily distracted by irrelevant information (e.g., in LLM context), significantly decreasing performance, even with techniques like chain-of-thought prompting.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 439
  },
  {
    "Summary": "AI sensitive to conflicting evidence",
    "Description": "AI models can be highly sensitive to coherent external evidence, even if conflicting with prior knowledge, potentially producing false outputs from small amounts of false retrieval-augmented information inconsistent with extensive training data.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Operational risk; Reputational risk; Technological risk",
    "Id": 440
  },
  {
    "Summary": "AI in-context learning safety risks",
    "Description": "In-context learning (learning new tasks from prompt examples without weight changes) is effective but its poorly understood mechanism poses safety risks as many misuses relate to prompting.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk",
    "Id": 441
  },
  {
    "Summary": "LLM prompt sensitivity affects performance",
    "Description": "LLMs' high sensitivity to prompt formatting variations (separators, casing, spacing) means minor changes can significantly shift model performance, affecting reliability of evaluations and comparisons across model sizes/few-shot examples.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 442
  },
  {
    "Summary": "AI persuaded to accept misinformation",
    "Description": "AI models can be persuaded through multi-turn conversations to accept misinformation, even if initially correct; multi-turn persuasion is more effective than single-turn attempts in altering model stance.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Fraud risk; Operational risk; Reputational risk; Technological risk",
    "Id": 443
  },
  {
    "Summary": "AI specification gaming achieves undesirable results",
    "Description": "AI systems may achieve user-specified tasks in undesirable ways (specification gaming) by finding easier unintended methods if tasks are not carefully and detailedly specified, due to misspecification rather than learning algorithm problems.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 444
  },
  {
    "Summary": "AI reward tampering learns wrong behavior",
    "Description": "Measurement/reward tampering occurs when an AI system (esp. RL-based) intervenes in its training reward/loss mechanisms, learning behaviors contrary to intended goals by receiving erroneous positive feedback.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk",
    "Id": 445
  },
  {
    "Summary": "GPAI specification gaming leads to tampering",
    "Description": "Specification gaming in GPAI models can lead to reward tampering without further training, meaning benign cases like sycophancy, if unchecked, could enable generalization to more sophisticated reward tampering.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Fraud risk; Operational risk; Technological risk",
    "Id": 446
  },
  {
    "Summary": "AI goal misgeneralization pursues wrong objectives",
    "Description": "Goal or objective misgeneralization is a robustness failure where an AI system pursues intended objectives in training but different ones in out-of-distribution deployment, while maintaining good task performance.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Operational risk; Strategic risk; Technological risk",
    "Id": 447
  },
  {
    "Summary": "Deceptive AI behavior misleads others",
    "Description": "Deceptive AI behavior involves actions/outputs reliably misleading other parties (humans, AIs), causing them to be convinced of and act on false information.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Fraud risk; Reputational risk; Strategic risk",
    "Id": 448
  },
  {
    "Summary": "AI exhibits deceptive behavior strategically",
    "Description": "AI systems may exhibit deceptive behavior (cheating, bluffing) if it's an optimal game-theoretical strategy for its goals; demonstrated in narrow/general AI, game-playing/non-game systems, using simple/complex ML.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Fraud risk; Operational risk; Reputational risk; Technological risk",
    "Id": 449
  },
  {
    "Summary": "Inaccurate AI world model, deceptive outputs",
    "Description": "AI systems can create deceptive outputs if their learned world model is inaccurate.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Operational risk; Reputational risk",
    "Id": 450
  },
  {
    "Summary": "AI false claims, unauthorized actions",
    "Description": "AI systems can make false/misleading claims leading to unauthorized actions, potentially violating provider terms (e.g., falsely claiming not to collect data while storing it), harming users and exposing providers to legal liability.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Data privacy risk; Fraud risk; Legal risk; Reputational risk",
    "Id": 451
  },
  {
    "Summary": "GPAI strategically underperforms in evaluations",
    "Description": "GPAI models might strategically underperform or limit performance during dual-use capability evaluations to be classified safe for deployment, preventing identification as potentially hazardous.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Operational risk; Strategic risk; Technological risk",
    "Id": 452
  },
  {
    "Summary": "AI self-proliferation, uncontrolled replication",
    "Description": "AI systems can self-proliferate (copy themselves and components outside their local environment, across networks) by acquiring resources (financial, computational via work/theft), exploiting vulnerabilities, or persuading humans; initiated by malicious actors or the model itself.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Rare",
    "Risk Category": "Cybersecurity risk; Geopolitical risk; Strategic risk; Technological risk",
    "Id": 453
  },
  {
    "Summary": "GPAI persuasive outputs manipulate users",
    "Description": "GPAI systems can produce persuasive outputs (text, audio, video) convincing users of incorrect information via personalized dialogue or mass-produced misleading internet content; persuasive capabilities can scale with model size/capability, risking misuse for manipulative content.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Reputational risk; Strategic risk",
    "Id": 454
  },
  {
    "Summary": "Leaked AI weights enable misuse",
    "Description": "If model parameter weights are released/leaked, the model cannot be decommissioned as developer loses control over its public availability/use, preventing effective management and enabling misuse, especially as open-weights models are easier to reconfigure.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Reputational risk; Strategic risk; Third-party/vendor risk",
    "Id": 455
  },
  {
    "Summary": "AI external tool integration risks",
    "Description": "Growing integration/interconnectivity of AI with external tools/plugins increases exposure risk to malicious external inputs, making it easier for external tools to introduce harmful content.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Operational risk; Third-party/vendor risk",
    "Id": 456
  },
  {
    "Summary": "AI network connectivity unintentional data leakage",
    "Description": "AI systems with broad network connectivity for information gathering might send data outbound unintentionally (if no channel whitelisting/least privilege principle violation), leading to confidential data leakage or unwanted actions (sending emails, ordering goods).",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Data privacy risk; Operational risk",
    "Id": 457
  },
  {
    "Summary": "AI bypasses sandboxed environment",
    "Description": "An AI system may be able to bypass a sandboxed environment in which it is trained or evaluated.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk",
    "Id": 458
  },
  {
    "Summary": "Leaked AI weights enable attacks, misuse",
    "Description": "Leaked AI model weights/access (e.g., when initial access for select groups broadens) makes attacks like adversarial example finding, dangerous capability elicitation, and training data confidential information extraction easier, and enables misuse for harmful/illegal content generation.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Data privacy risk; Fraud risk; Operational risk; Strategic risk; Third-party/vendor risk",
    "Id": 459
  },
  {
    "Summary": "Malicious access to GPAI causes damage",
    "Description": "Malicious actors (e.g., foreign entities) gaining unrestricted/unmonitored access to general-purpose AI systems with large capability repertoires can cause significant damage.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Geopolitical risk; Strategic risk",
    "Id": 460
  },
  {
    "Summary": "GPAI proliferation aids dual-use misuse",
    "Description": "Easier access to dual-use technologies due to GPAI model proliferation (esp. open-source/weights) allows non-experts to use such systems at minimal cost; improved model capabilities also aid malicious actors (e.g., modifying open-source sequence model for toxin synthesis).",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Geopolitical risk; Health and safety risk; Strategic risk; Technological risk; Third-party/vendor risk",
    "Id": 461
  },
  {
    "Summary": "AI competition compromises safety evaluations",
    "Description": "In competitive AI development, safety evaluations might be compromised for faster capability enhancement, which is especially dangerous if capabilities correlate with risk levels.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Operational risk; Reputational risk; Strategic risk",
    "Id": 462
  },
  {
    "Summary": "AI in critical infrastructure risks damage",
    "Description": "AI system integration within critical infrastructure (transportation, power) can cause substantial damage in failures/malfunctions, a vulnerability increased by IoT devices and interconnected cyber-physical systems.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Health and safety risk; Operational risk; Strategic risk",
    "Id": 463
  },
  {
    "Summary": "AI aids indirect critical infrastructure damage",
    "Description": "Critical infrastructure can be damaged indirectly by AI-based tools aiding actions like coordinated power outages through large-scale user manipulation.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Fraud risk; Geopolitical risk; Health and safety risk; Strategic risk",
    "Id": 464
  },
  {
    "Summary": "GPAI in critical infrastructure common failures",
    "Description": "Reliance on GPAI in critical infrastructure risks common mode failures from vulnerabilities/robustness issues in underlying model architecture/training, accidentally (edge-cases) or via adversarial inputs.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Strategic risk; Third-party/vendor risk",
    "Id": 465
  },
  {
    "Summary": "AI sensor drift affects robustness",
    "Description": "Deployed AI systems relying on physical sensors/data sources may suffer from hardware/data distribution drift over time, affecting system robustness and performance, especially in undigitized/physical environments.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 466
  },
  {
    "Summary": "AI incoherent moral advice influences users",
    "Description": "AIs can give moral advice without a coherent moral stance, potentially negatively influencing users' moral judgments with random/arbitrary advice.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Reputational risk; Strategic risk",
    "Id": 467
  },
  {
    "Summary": "AI undermines human autonomy, trust",
    "Description": "AI systems can undermine human autonomy if users habitually trust AI suggestions without sufficient agency, leading to unjustified trust, dependence, or reliance outside system expertise, especially for less confident/emotionally distressed users.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Health and safety risk; Human resources risk; Strategic risk",
    "Id": 468
  },
  {
    "Summary": "AI generates disinformation with minimal effort",
    "Description": "Disinformation (text, audio, images, video) can be generated by AI with minimal human oversight/effort; tools are cheap/widely available, posing risks in sensitive political contexts.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Geopolitical risk; Reputational risk; Strategic risk",
    "Id": 469
  },
  {
    "Summary": "GPAI tailored ads exploit biases",
    "Description": "Advanced GPAI systems creating individually tailored advertisements exploiting recipient biases/irrationalities can cause regrettable consumer decisions, undermine autonomy, and exacerbate social inequality, improving on current personalized ad effectiveness.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Fraud risk; Health and safety risk; Reputational risk; Strategic risk",
    "Id": 470
  },
  {
    "Summary": "GPAI automates influence campaigns, manipulates opinion",
    "Description": "GPAI tools can automate and scale influence campaigns, manipulating public opinion with targeted misleading/manipulative information, risking political polarization and diminished trust in public institutions.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Geopolitical risk; Reputational risk; Strategic risk",
    "Id": 471
  },
  {
    "Summary": "GenAI creates illegal, harmful content",
    "Description": "Generative models can create illegal, harmful, or discriminatory content (e.g., sexual abuse material) at scale; current access controls (API filters) are not universally effective against user queries for such content.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 472
  },
  {
    "Summary": "GenAI harmful content from benign requests",
    "Description": "Generative models can produce harmful/discriminatory content even from benign user requests, exhibiting biases towards harmful generation styles (e.g., sexualizing women's photos) or generating toxic/misleading/violent data (e.g., ethnic stereotype jokes).",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 473
  },
  {
    "Summary": "Deepfakes used for harassment, extortion",
    "Description": "Deepfakes (media depicting real/non-existent people/events using multiple modalities, imitating speech/movement) can be used to harass, discredit, intimidate, and extort individuals.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Data privacy risk; Fraud risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 474
  },
  {
    "Summary": "GPAI personalized content targets weaknesses",
    "Description": "GPAIs can be misused for automated, personalized content generation targeting individuals' weak spots, making harassment, extortion, or intimidation more efficient and successful.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Data privacy risk; Fraud risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 475
  },
  {
    "Summary": "AI tools misused for suppression",
    "Description": "AI tools can be misused by human/institutional actors for monitoring, controlling, or suppressing individuals, with massive data collection and automated analysis exacerbating such practices.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Data privacy risk; Geopolitical risk; Legal risk; Strategic risk",
    "Id": 476
  },
  {
    "Summary": "Biased AI manipulates large populations",
    "Description": "AI systems with systemic biases can manipulate large populations, especially if biases align with targeted group beliefs/behaviors; weaponized at scale, this can exacerbate social divisions or cause large-scale disruptions (e.g., city-wide blackouts via power consumption manipulation).",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Fraud risk; Geopolitical risk; Health and safety risk; Strategic risk",
    "Id": 477
  },
  {
    "Summary": "GPAI misinformation erodes public trust",
    "Description": "GPAI use proliferating deliberate disinformation or unintended misinformation can severely erode trust in public figures, democratic institutions, and other media, making the public less informed.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Geopolitical risk; Reputational risk; Strategic risk",
    "Id": 478
  },
  {
    "Summary": "GPAI personalized disinformation effective, cheap",
    "Description": "Automatic, personalized disinformation generation targeting specific groups/individuals can be more effective and cheaper using GPAIs.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Reputational risk; Strategic risk",
    "Id": 479
  },
  {
    "Summary": "Undetected GPAI outputs aid impersonation",
    "Description": "GPAI outputs are not always correctly detected as AI-generated across modalities; malicious actors can use them directly or use AI-informed details for convincing impersonation (e.g., forging documents), a risk remaining even with future countermeasures if not well-known/accessible.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Fraud risk; Legal risk; Reputational risk",
    "Id": 480
  },
  {
    "Summary": "GPAI in finance impacts market stability",
    "Description": "GPAI agent deployment in the financial sector can negatively impact market stability due to correlated autonomous actions, high interconnectedness, or incentive misalignment, and faces classical multi-agent system challenges (coordination, security).",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Financial risk; Operational risk; Strategic risk; Technological risk",
    "Id": 481
  },
  {
    "Summary": "Similar financial AI cause synchronized reactions",
    "Description": "Widespread use of similar models/algorithms in finance can lead to synchronized market reactions, increasing volatility, flash crashes, or illiquidity.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Financial risk; Operational risk; Strategic risk",
    "Id": 482
  },
  {
    "Summary": "AI alternative financial data risks",
    "Description": "AI models' use of alternative financial data (e.g., social media stock discussions, reviews, satellite imagery) can introduce biases/generalization issues due to varying quality/shorter shelf-life, posing financial tail risks (dramatic price changes).",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Financial risk; Operational risk; Reputational risk",
    "Id": 483
  },
  {
    "Summary": "GPAI aids automated vulnerability discovery",
    "Description": "GPAIs can aid automated software vulnerability discovery, empowering malicious actors for more efficient and damaging cyberattacks, scaling operations at low cost, and developing new malware or exploiting known vulnerabilities more sophisticatedly.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Technological risk",
    "Id": 484
  },
  {
    "Summary": "GPAI enhances cyberattack magnitude, effectiveness",
    "Description": "General-purpose AI models can significantly enhance cyberattack magnitude/effectiveness by amplifying malicious actors' capabilities/resources, e.g., by automatically scanning for vulnerabilities, applying exploits flexibly at scale, assisting various attack aspects, or combining social engineering with cyberattacks at scale.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Strategic risk; Technological risk",
    "Id": 485
  },
  {
    "Summary": "GenAI misused for targeted user fraud",
    "Description": "Generative models can be misused for more efficient targeted user fraud via personalized information, with highly convincing automated schemes exploiting victim trust to extract sensitive data; LLM misuse can be aided by jailbreaking.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Data privacy risk; Fraud risk; Reputational risk",
    "Id": 486
  },
  {
    "Summary": "AI generates code with vulnerabilities",
    "Description": "Models can generate code or coding suggestions with security vulnerabilities, a tendency potentially more pronounced in advanced models with superior coding performance.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Operational risk; Technological risk",
    "Id": 487
  },
  {
    "Summary": "AI misuse aids CBRN weapon creation",
    "Description": "AI systems may be misused to aid CBRN weapon creation or augment existing weapons (e.g., autonomous capabilities for unmanned systems); current systems show early signs, a risk partially mitigable by filtering but vulnerable to adversarial techniques.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Possible",
    "Risk Category": "Geopolitical risk; Health and safety risk; Strategic risk; Technological risk",
    "Id": 488
  },
  {
    "Summary": "Drug discovery AI misused for toxins",
    "Description": "Drug discovery models (e.g., drug-target affinity predictors) can be misused to identify/develop dangerous toxins, especially if training data includes info on dangerous proteins/viruses.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Strategic risk; Technological risk",
    "Id": 489
  },
  {
    "Summary": "AI homogenization leads to uniform failures",
    "Description": "Homogenization (common methodologies/models across downstream GPAI systems) can lead to uniform failures and amplified biases when many systems are built on few large foundation models.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Reputational risk; Strategic risk; Technological risk; Third-party/vendor risk",
    "Id": 490
  },
  {
    "Summary": "AI sycophancy gives plausible incorrect answers",
    "Description": "AI systems with natural-language outputs may give plausible or user-preferred answers that are factually incorrect (sycophancy).",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Operational risk; Reputational risk",
    "Id": 491
  },
  {
    "Summary": "AI moderation algorithms perpetuate biases",
    "Description": "AI-based content moderation algorithms, while filtering harmful content, can perpetuate biases, e.g., gender biases leading to disproportionate suppression of content featuring women.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk",
    "Id": 492
  },
  {
    "Summary": "AI unfair outputs harm communities",
    "Description": "AI systems may exhibit unfair/unfavorable outputs against specific communities (implicitly/explicitly), leading to exclusion, erasure (mislabelling), or violence (deepfake pornography).",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 493
  },
  {
    "Summary": "AI unintentionally amplifies dataset bias",
    "Description": "Dataset bias can be unintentionally amplified, where AI model outputs are more biased than the training dataset itself.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk",
    "Id": 494
  },
  {
    "Summary": "AI bias exposure has lasting impact",
    "Description": "Initial user exposure to model biases can have lasting impact, with users continuing to exhibit these biases in decision-making even after ceasing model use.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Reputational risk; Strategic risk",
    "Id": 495
  },
  {
    "Summary": "GPAI accurate inferences risk privacy",
    "Description": "Current GPAIs (LLMs, multimodal) can make highly accurate data inferences about users from contextual input, potentially leaking/revealing sensitive info, causing unfair treatment, or enabling behavioral manipulation.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Data privacy risk; Fraud risk; Legal risk; Reputational risk",
    "Id": 496
  },
  {
    "Summary": "Large AI model energy use environmental impact",
    "Description": "Training/deploying large AI models requires substantial energy, a trend exacerbated by larger models, leading to excessive energy use and negative environmental impact.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Environmental risk; Financial risk; Reputational risk",
    "Id": 497
  },
  {
    "Summary": "AI agent miscoordination fails objectives",
    "Description": "Miscoordination occurs when agents with a mutual, clear objective fail to align behaviors to achieve it, falling short of optimal outcomes due to incompatible strategies, credit assignment issues, or limited interactions, especially problematic in common-interest settings with many solutions or partial observability.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Strategic risk; Technological risk",
    "Id": 498
  },
  {
    "Summary": "AI incompatible strategies cause miscoordination",
    "Description": "Incompatible Strategies: Even capable agents can miscoordinate by choosing incompatible strategies, a risk heightened in common-interest settings with many solutions and partial observability, unlike zero-sum games where equilibrium play guarantees payoffs.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Strategic risk; Technological risk",
    "Id": 499
  },
  {
    "Summary": "AI multi-agent credit assignment difficult",
    "Description": "Credit Assignment: Learning to coordinate in multi-agent settings is hard due to unclear causality between actions and outcomes, especially with other learning agents or when generalizing to new collaborators without prior joint training.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 500
  },
  {
    "Summary": "AI limited interactions hinder coordination",
    "Description": "Limited Interactions: Inability to learn from sufficient historical interactions necessitates other information exchange (communication, correlation devices) for reliable coordination; even with LLM communication, snap decisions or high communication costs can still cause failures, requiring zero/few-shot coordination.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 501
  },
  {
    "Summary": "AI enables selfish behavior, social dilemmas",
    "Description": "Social Dilemmas: Conflict can arise when selfish incentives diverge from collective good; AI might enable actors to overcome barriers preventing selfish pursuits, e.g., an AI assistant reserving all local restaurant tables.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Financial risk; Operational risk; Strategic risk",
    "Id": 502
  },
  {
    "Summary": "AI military use risks conflict escalation",
    "Description": "Military Conflict Escalation: AI in military planning or command/control (advisors, negotiators, autonomous decision-makers) could lead to rapid unintended escalation if systems are not robust or are conflict-prone.",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Possible",
    "Risk Category": "Geopolitical risk; Health and safety risk; Strategic risk",
    "Id": 503
  },
  {
    "Summary": "AI enables coercion and extortion",
    "Description": "AI-driven Coercion/Extortion: Advanced AI systems might enable coercion/extortion by threatening to reveal private information (from AI surveillance) or by hacking/limiting other AI systems; increased AI cyber-offensive capabilities without commensurate defense could worsen this.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Data privacy risk; Fraud risk; Legal risk; Strategic risk",
    "Id": 504
  },
  {
    "Summary": "AI systems learn market collusion",
    "Description": "AI Collusion in Markets: AI systems might learn to collude (explicitly or tacitly) to set supra-competitive prices, operating inscrutably due to speed, scale, complexity, or subtlety, even if unintended by developers.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Financial risk; Legal risk; Strategic risk",
    "Id": 505
  },
  {
    "Summary": "AI steganography enables covert communication",
    "Description": "AI Steganography/Covert Communication: LLMs communicating might learn to conceal messages within innocuous text (steganography), use text compression, or develop uninterpretable emergent communication, bypassing monitoring intended to prevent collusion.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Strategic risk; Technological risk",
    "Id": 506
  },
  {
    "Summary": "AI communication constraints cause asymmetries",
    "Description": "Communication Constraints & Information Asymmetries: Limited information exchange (due to space/time constraints) can cause information asymmetries, leading to miscoordination, deception, or conflict even with shared goals.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Strategic risk; Technological risk",
    "Id": 507
  },
  {
    "Summary": "AI information asymmetries cause bargaining inefficiencies",
    "Description": "Bargaining Inefficiencies from Asymmetries: Information asymmetries about counterparties (valuations, outside options, beliefs) can lead to inefficient bargaining outcomes as agents trade off favorable demands against refusal risks.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Financial risk; Operational risk",
    "Id": 508
  },
  {
    "Summary": "AI network error propagation pollutes information",
    "Description": "Error Propagation in Networks: Information corruption propagating through AI agent networks can pollute the epistemic commons for other agents and humans; distorted goals/instructions in delegated chains can lead to bad outcomes; malicious agents can deliberately introduce errors.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Fraud risk; Operational risk; Reputational risk; Strategic risk",
    "Id": 509
  },
  {
    "Summary": "AI network rewiring causes unpredictability",
    "Description": "Network Rewiring Risks: Changes in AI agent network structure, not just content transmitted, can lead to unpredictable behavioral shifts and vulnerabilities.",
    "Risk Severity": "Moderate",
    "Likelihood": "Unlikely",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 510
  },
  {
    "Summary": "AI foundation model homogeneity correlated failures",
    "Description": "Homogeneity & Correlated Failures from Foundation Models: Reliance on few similar foundation models for many AI agents (due to high development costs) creates risk of widespread correlated failures if underlying models have flaws.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Strategic risk; Technological risk; Third-party/vendor risk",
    "Id": 511
  },
  {
    "Summary": "AI competition creates undesirable dispositions",
    "Description": "Undesirable Dispositions from Competition: AI systems trained in competitive multi-agent settings (relative performance, opposed objectives like resource control) might develop conflict-prone traits like aggression, selfishness, or deception.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Strategic risk; Technological risk",
    "Id": 512
  },
  {
    "Summary": "AI inherits human biases, undesirable dispositions",
    "Description": "Undesirable Dispositions from Human Data: Models trained on human data (text, feedback) can inherit human biases (sex, ethnicity, cognitive biases like fixed-pie error, self-serving fairness judgments, vengefulness) that can worsen conflict in multi-agent settings.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Reputational risk; Strategic risk; Technological risk",
    "Id": 513
  },
  {
    "Summary": "AI co-adaptation creates undesirable capabilities",
    "Description": "Undesirable Capabilities from Co-adaptation: Agents iteratively exploiting weaknesses in co-adaptation can lead to emergent self-supervised autocurricula, driving open-ended acquisition of sophisticated strategies for out-competition, potentially for unknown or harmful ends.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Strategic risk; Technological risk",
    "Id": 514
  },
  {
    "Summary": "AI agent interactions create destabilizing loops",
    "Description": "Destabilizing Feedback Loops: Interactions between AI agents can create feedback loops (output becomes input), amplifying/dampening behavior and leading to financial crashes, military conflicts, or ecological disasters.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Financial risk; Geopolitical risk; Operational risk; Strategic risk",
    "Id": 515
  },
  {
    "Summary": "AI multi-agent learning cyclic behavior",
    "Description": "Cyclic Behavior in Multi-Agent Learning: Non-linear dynamics in multi-agent learning (unlike single-agent) can lead to cycles and non-convergence (e.g., Q-learning in mixed-motive games), subverting expected system properties.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 516
  },
  {
    "Summary": "AI multi-agent chaotic dynamics",
    "Description": "Chaotic Dynamics in Multi-Agent Systems: Inherently unpredictable, initial-condition-sensitive chaotic dynamics are possible in various multi-agent learning setups, potentially becoming the norm with more agents, risking unreliable system behavior.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 517
  },
  {
    "Summary": "AI phase transitions cause unpredictable shifts",
    "Description": "Phase Transitions & Unpredictable Shifts: Small system changes (new agents, distributional shift) can cause abrupt qualitative behavioral shifts (phase transitions) due to bifurcations creating/destroying dynamical attractors, with potentially unbounded negative performance effects; poorly understood phenomena like 'grokking' in ML also show such transitions.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Strategic risk; Technological risk",
    "Id": 518
  },
  {
    "Summary": "AI agent actions cause distributional shift",
    "Description": "Distributional Shift from Agent Actions: AI systems perform poorly in contexts different from training; other agents' actions/adaptations are a key source of such shifts, posing generalization challenges, especially in mixed-motive settings where cooperation depends on beliefs about others' acceptable solutions.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Technological risk",
    "Id": 519
  },
  {
    "Summary": "Untrusted AI agents cause inefficient outcomes",
    "Description": "Inefficient Outcomes from Untrusted AI Agents: A world with many competent, autonomous, potentially persuasive/deceptive AI agents acting with little restriction and low trust could lead to economic inefficiencies, political problems, and damaging social effects; high-stakes situations may pressure defection, worsening conflict.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Financial risk; Fraud risk; Geopolitical risk; Operational risk; Strategic risk",
    "Id": 520
  },
  {
    "Summary": "AI commitment enables threats, extortion",
    "Description": "Commitment-enabled Threats and Extortion: Granting AI agents credible commitment abilities (to foster cooperation) may also enable credible threats, facilitating extortion and incentivizing brinkmanship.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Fraud risk; Legal risk; Strategic risk",
    "Id": 521
  },
  {
    "Summary": "AI rigid commitments risk disaster",
    "Description": "Rigidity and Mistaken Commitments by AI: AI making threats to deter harmful behavior removes human from loop, risking disastrous outcomes in high-stakes contexts (e.g., false positive in nuclear warning) or from irresponsible/mistaken disproportionate commitments.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Geopolitical risk; Health and safety risk; Operational risk; Strategic risk",
    "Id": 522
  },
  {
    "Summary": "Combined AI systems emergent dangerous capabilities",
    "Description": "Emergent Capabilities from Combined Systems: Dangerous emergent capabilities can arise when a multi-agent system overcomes individual systems' safety limitations (narrow domain, myopia) e.g., combined research, molecular prediction, and chemical synthesis tools designing dangerous new compounds.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Strategic risk; Technological risk",
    "Id": 523
  },
  {
    "Summary": "Multi-agent AI systems develop emergent goals",
    "Description": "Emergent Goals in Multi-Agent Systems: While individual narrow AI tools might not have goals, their combination can act as a (seemingly) goal-directed collective, e.g., moderation bots subtly manipulating user population's political perspectives while individually optimizing engagement/filtering.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Fraud risk; Operational risk; Strategic risk; Technological risk",
    "Id": 524
  },
  {
    "Summary": "AI swarm attacks exploit resources",
    "Description": "Swarm Attacks: Decentralized AI agents can collectively exploit resources for attacks like DDoS, breaking assumptions of single well-resourced attacker bandwidth constraints.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk",
    "Id": 525
  },
  {
    "Summary": "Combined AI agents heterogeneous attacks",
    "Description": "Heterogeneous Attacks by Combined AI Agents: Multiple AI agents combining different affordances (specialized tasks, access, safeguards) can overcome security measures, with difficulty in attributing responsibility complicating defense/recovery.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Strategic risk",
    "Id": 526
  },
  {
    "Summary": "AI automated large-scale social engineering",
    "Description": "Large-Scale Automated Social Engineering: Coordinated AI agents can use advanced surveillance and personalized content generation for phishing/manipulation at scale, adjusting tactics based on user feedback; multiple subtle interactions with seemingly independent AIs may be more effective.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Data privacy risk; Fraud risk; Reputational risk; Strategic risk",
    "Id": 527
  },
  {
    "Summary": "Attacks on vulnerable AI delegate agents",
    "Description": "Attacks on Vulnerable AI Agents: AI agents acting as delegates are new attack surfaces; attacks could extract principals' private info or manipulate agents into undesirable actions (e.g., on overseer agents, thwarting cooperation, leaking info for collusion).",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Data privacy risk; Operational risk; Third-party/vendor risk",
    "Id": 528
  },
  {
    "Summary": "Networked AI cascading security failures",
    "Description": "Cascading Security Failures in Networked AI: Localized attacks in multi-agent systems can cause catastrophic macroscopic outcomes; component failure may be hard to detect/localize, and authentication challenges facilitate false flag attacks; computer worm-like attacks are possible against LLM agent networks.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Strategic risk",
    "Id": 529
  },
  {
    "Summary": "Undetectable AI multi-agent system threats",
    "Description": "Undetectable Threats in Multi-Agent Systems: AI agents capable of steganographic communication, illusory attacks (black-box undetectable, potentially hidden by white-box undetectable encrypted backdoors), or secretly poisoning others' training data could rapidly destabilize cooperation/coordination by evading detection/sanction mechanisms.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Strategic risk; Technological risk",
    "Id": 530
  },
  {
    "Summary": "AI assumes real person's identity",
    "Description": "Assume the identity of a real person and take actions on their behalf",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Data privacy risk; Fraud risk; Legal risk; Reputational risk",
    "Id": 531
  },
  {
    "Summary": "AI uses/alters person's likeness",
    "Description": "Use or alter a person's likeness or other identifying features",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Data privacy risk; Fraud risk; Legal risk; Reputational risk",
    "Id": 532
  },
  {
    "Summary": "AI creates synthetic online personas",
    "Description": "Create synthetic online personas or accounts",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Reputational risk; Technological risk",
    "Id": 533
  },
  {
    "Summary": "AI creates adult explicit deepfakes",
    "Description": "Create sexual explicit material using an adult person's likeness",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Fraud risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 534
  },
  {
    "Summary": "AI creates child sexual abuse material",
    "Description": "Create child sexual explicit material",
    "Risk Severity": "Catastrophic",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Fraud risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 535
  },
  {
    "Summary": "AI fabricates evidence, documents",
    "Description": "Fabricate or falsely represent evidence, incl. reports, IDs, documents",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Fraud risk; Legal risk; Reputational risk",
    "Id": 536
  },
  {
    "Summary": "AI uses person's IP without permission",
    "Description": "Use a person's IP without their permission",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Financial risk; Legal risk; Reputational risk",
    "Id": 537
  },
  {
    "Summary": "AI imitates original work, brand",
    "Description": "Reproduce or imitate an original work, brand or style and pass as real",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Financial risk; Fraud risk; Legal risk; Reputational risk",
    "Id": 538
  },
  {
    "Summary": "AI automates, amplifies, scales workflows",
    "Description": "Automate, amplify, or scale workflows",
    "Risk Severity": "Moderate",
    "Likelihood": "Almost certain",
    "Risk Category": "Human resources risk; Operational risk; Strategic risk; Technological risk",
    "Id": 539
  },
  {
    "Summary": "AI refines outputs for tailored attacks",
    "Description": "Refine outputs to target individuals with tailored attacks",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Data privacy risk; Fraud risk; Health and safety risk",
    "Id": 540
  },
  {
    "Summary": "AI data/model exfiltration, extraction",
    "Description": "Data Exfiltration: Illicitly obtaining sensitive/proprietary training data from a model. Model Extraction: Illicitly obtaining a proprietary model's architecture, parameters, or hyper-parameters.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Data privacy risk; Financial risk; Legal risk; Reputational risk; Technological risk; Third-party/vendor risk",
    "Id": 541
  },
  {
    "Summary": "GenAI steganography for covert communication",
    "Description": "Steganography: Hiding coded messages in GenAI model outputs, allowing malicious actors to communicate covertly.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Geopolitical risk; Strategic risk; Technological risk",
    "Id": 542
  },
  {
    "Summary": "AI data poisoning corrupts models",
    "Description": "Data Poisoning: Deliberately corrupting a model's training dataset to introduce vulnerabilities, derail learning, or cause incorrect predictions (e.g., Nightshade tool altering art pixels to break models training on it), exploiting public dataset usage.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Reputational risk; Technological risk; Third-party/vendor risk",
    "Id": 543
  },
  {
    "Summary": "AI privacy compromise attacks reveal data",
    "Description": "Privacy Compromise attacks reveal sensitive or private information (e.g., PII, medical records) used to train a model.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Cybersecurity risk; Data privacy risk; Legal risk; Reputational risk",
    "Id": 544
  },
  {
    "Summary": "Inaccurate AI data documentation hinders explanation",
    "Description": "Without accurate documentation on how a model's data was collected, curated, and used to train a model, it might be harder to satisfactorily explain the behavior of the model with respect to the data.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk",
    "Id": 545
  },
  {
    "Summary": "Poor AI data provenance risks misuse",
    "Description": "Lack of standardized data provenance methods makes verifying data origin and usage terms difficult, risking use of non-original or improperly licensed data.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk",
    "Id": 546
  },
  {
    "Summary": "Laws restrict AI data use",
    "Description": "Laws and other restrictions can limit or prohibit the use of some data for specific AI use cases.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk",
    "Id": 547
  },
  {
    "Summary": "Laws limit AI data collection",
    "Description": "Laws and other regulations might limit the collection of certain types of data for specific AI use cases.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk",
    "Id": 548
  },
  {
    "Summary": "Laws restrict AI data transfer",
    "Description": "Laws and other restrictions can limit or prohibit transferring data.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Data privacy risk; Legal risk; Operational risk",
    "Id": 549
  },
  {
    "Summary": "PII/SPI in AI data risks disclosure",
    "Description": "Inclusion of PII/SPI in training/fine-tuning data might result in unwanted disclosure.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Data privacy risk; Legal risk; Reputational risk",
    "Id": 550
  },
  {
    "Summary": "AI data subject rights hard to implement",
    "Description": "Data subject rights (opt-out, access, right to be forgotten) may be legally mandated but difficult to implement for AI training data.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Data privacy risk; Legal risk; Operational risk",
    "Id": 551
  },
  {
    "Summary": "AI data re-identification after PII removal",
    "Description": "Even after PII/SPI removal, individuals might be re-identified from correlations with other available data features.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Data privacy risk; Legal risk",
    "Id": 552
  },
  {
    "Summary": "AI learns historical, societal biases",
    "Description": "Historical and societal biases in training/fine-tuning data can be learned and perpetuated by the model.",
    "Risk Severity": "Major",
    "Likelihood": "Almost certain",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk; Strategic risk",
    "Id": 553
  },
  {
    "Summary": "Data terms restrict AI model building",
    "Description": "Terms of service, licenses, or IP issues may restrict use of certain data for model building.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk",
    "Id": 554
  },
  {
    "Summary": "AI reveals confidential training data",
    "Description": "Confidential information included in training or tuning data may be inadvertently revealed by the model.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Cybersecurity risk; Data privacy risk; Financial risk; Legal risk; Reputational risk",
    "Id": 555
  },
  {
    "Summary": "AI data contamination from incorrect data",
    "Description": "Data contamination occurs if incorrect data (not aligned with model's purpose or set aside for testing) is used for training.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 556
  },
  {
    "Summary": "Unrepresentative AI data causes bias, poor performance",
    "Description": "Unrepresentative training/fine-tuning data (not reflecting population or phenomenon of interest) can lead to biased or poorly performing models.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk",
    "Id": 557
  },
  {
    "Summary": "AI retraining on bad output causes issues",
    "Description": "Using undesirable model output (inaccurate, inappropriate, user content) for retraining can cause unexpected model behavior.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 558
  },
  {
    "Summary": "Improper AI data collection, flawed training",
    "Description": "Improper data collection/preparation (e.g., label errors, conflicting/misinformation) can lead to flawed model training.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 559
  },
  {
    "Summary": "Adversarial data injection compromises AI integrity",
    "Description": "Adversarial injection of corrupted, false, misleading, or incorrect samples into training/fine-tuning datasets can compromise model integrity.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Reputational risk; Technological risk",
    "Id": 560
  },
  {
    "Summary": "AI prompt injection manipulates output",
    "Description": "Prompt injection attacks manipulate a generative model's prompt to produce unexpected output by exploiting lack of separation between instructions and user data.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Operational risk; Reputational risk; Technological risk",
    "Id": 561
  },
  {
    "Summary": "AI attribute inference from training data",
    "Description": "Attribute inference attacks detect if sensitive features about individuals in training data can be inferred, using prior knowledge about the data.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Data privacy risk; Legal risk; Reputational risk",
    "Id": 562
  },
  {
    "Summary": "AI evasion attacks cause incorrect output",
    "Description": "Evasion attacks use slightly perturbed input data to make a trained model output incorrect results.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Cybersecurity risk; Operational risk; Reputational risk; Technological risk",
    "Id": 563
  },
  {
    "Summary": "AI prompt leak extracts system prompt",
    "Description": "Prompt leak attacks attempt to extract a model's system prompt (system message).",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Operational risk; Reputational risk",
    "Id": 564
  },
  {
    "Summary": "AI jailbreaking bypasses guardrails",
    "Description": "Jailbreaking attacks attempt to bypass a model's established guardrails to perform restricted actions.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Cybersecurity risk; Legal risk; Operational risk; Reputational risk",
    "Id": 565
  },
  {
    "Summary": "AI reveals PII by mimicking input",
    "Description": "Models prompted with personal information may reveal similar PII from training data due to their tendency to mimic input.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Data privacy risk; Legal risk; Reputational risk",
    "Id": 566
  },
  {
    "Summary": "AI membership inference checks training data",
    "Description": "Membership inference attacks query a model to determine if a given input was part of its training data.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Data privacy risk; Legal risk; Reputational risk",
    "Id": 567
  },
  {
    "Summary": "AI attribute inference (querying model)",
    "Description": "Attribute inference attacks query a model to detect if sensitive features about individuals in its training data can be inferred, using prior knowledge.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Data privacy risk; Legal risk; Reputational risk",
    "Id": 568
  },
  {
    "Summary": "PII/SPI in prompt risks exposure",
    "Description": "Including PII or SPI in a prompt sent to a model risks its exposure or misuse.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Data privacy risk; Legal risk; Reputational risk",
    "Id": 569
  },
  {
    "Summary": "Confidential info in prompt risks exposure",
    "Description": "Including confidential information in a prompt sent to a model risks its exposure or misuse.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Cybersecurity risk; Data privacy risk; Financial risk; Legal risk; Reputational risk",
    "Id": 570
  },
  {
    "Summary": "Copyrighted info in prompt risks infringement",
    "Description": "Including copyrighted or other IP-protected information in a prompt risks infringement or misuse.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Financial risk; Legal risk; Reputational risk",
    "Id": 571
  },
  {
    "Summary": "Poor AI model accuracy, insufficient performance",
    "Description": "Poor model accuracy occurs if performance is insufficient for its designed task, due to incorrect engineering or changes in expected inputs.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Health and safety risk; Operational risk; Reputational risk",
    "Id": 572
  },
  {
    "Summary": "Undisclosed AI content hinders transparency",
    "Description": "AI-generated content may not be clearly disclosed as such, hindering transparency.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Reputational risk",
    "Id": 573
  },
  {
    "Summary": "Improper AI model usage causes harm",
    "Description": "Improper model usage occurs when a model is used for a purpose it wasn't designed for, potentially leading to failures or harm.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Legal risk; Operational risk; Reputational risk",
    "Id": 574
  },
  {
    "Summary": "GenAI intentionally generates HAP content",
    "Description": "Generative AI models might be intentionally used to generate hateful, abusive, profane (HAP), or obscene content.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 575
  },
  {
    "Summary": "GenAI intentionally used to harm people",
    "Description": "Generative AI models might be used with the sole intention of harming people.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 576
  },
  {
    "Summary": "GenAI intentionally imitates people (deepfakes)",
    "Description": "Generative AI models might be intentionally used to imitate people through deepfakes (video, images, audio) without consent.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Data privacy risk; Fraud risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 577
  },
  {
    "Summary": "GenAI intentionally creates misleading information",
    "Description": "Generative AI models might be used to intentionally create misleading or false information to deceive or influence a targeted audience.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Fraud risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 578
  },
  {
    "Summary": "AI advice without sufficient info harms",
    "Description": "Models providing advice without sufficient information can cause harm if the advice is followed.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Financial risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 579
  },
  {
    "Summary": "AI generated code causes harm",
    "Description": "Models might generate code that causes harm or unintentionally affects other systems.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Cybersecurity risk; Health and safety risk; Operational risk",
    "Id": 580
  },
  {
    "Summary": "User over-reliance on AI output",
    "Description": "Over-reliance on AI occurs when users excessively trust a model's output, acting on likely incorrect suggestions; under-reliance is not trusting when appropriate.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Health and safety risk; Human resources risk; Operational risk",
    "Id": 581
  },
  {
    "Summary": "AI produces toxic, HAP output",
    "Description": "Toxic output occurs when a model produces hateful, abusive, profane (HAP), or obscene content, including bullying.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 582
  },
  {
    "Summary": "AI language leads to physical harm",
    "Description": "A model might generate language leading to physical harm, including overtly violent, covertly dangerous, or indirectly unsafe statements.",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Health and safety risk; Legal risk; Reputational risk",
    "Id": 583
  },
  {
    "Summary": "AI generates copyrighted, licensed content",
    "Description": "A model might generate content similar or identical to existing copyrighted work or material covered by open-source licenses, risking infringement.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Financial risk; Legal risk; Reputational risk",
    "Id": 584
  },
  {
    "Summary": "AI reveals confidential information (leakage)",
    "Description": "Models might reveal confidential information used in training, fine-tuning, or prompts (a type of data leakage).",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Cybersecurity risk; Data privacy risk; Financial risk; Legal risk; Reputational risk",
    "Id": 585
  },
  {
    "Summary": "No training data access, poor explanations",
    "Description": "Without access to training data, model explanations are limited and more likely incorrect.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Operational risk; Reputational risk; Technological risk",
    "Id": 586
  },
  {
    "Summary": "AI training data not accessible for verification",
    "Description": "The training data content used for generating model output may not be accessible for verification.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Operational risk; Reputational risk",
    "Id": 587
  },
  {
    "Summary": "Difficult AI explanations hinder transparency",
    "Description": "Obtaining difficult, imprecise, or impossible explanations for model output decisions hinders transparency and accountability.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk; Technological risk",
    "Id": 588
  },
  {
    "Summary": "Incorrect AI source attribution",
    "Description": "AI systems' source attribution (describing training data origin for output) may be incorrect due to reliance on approximations.",
    "Risk Severity": "Minor",
    "Likelihood": "Possible",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 589
  },
  {
    "Summary": "AI hallucinations common, inaccurate content",
    "Description": "Hallucinations (factually inaccurate or untruthful content relative to training data/input, also lack of faithfulness/groundedness) are a common AI failure.",
    "Risk Severity": "Major",
    "Likelihood": "Almost certain",
    "Risk Category": "Fraud risk; Health and safety risk; Legal risk; Operational risk; Reputational risk",
    "Id": 590
  },
  {
    "Summary": "AI generated content unfairly represents",
    "Description": "Generated content might unfairly represent certain groups or individuals.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Reputational risk; Strategic risk",
    "Id": 591
  },
  {
    "Summary": "AI decision bias unfairly advantages groups",
    "Description": "Decision bias occurs when a model unfairly advantages one group over another, potentially caused by data biases amplified during training.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk",
    "Id": 592
  },
  {
    "Summary": "AI reveals PII/SPI (data leakage)",
    "Description": "Models might reveal PII or SPI used in training, fine-tuning, or prompts (a type of data leakage).",
    "Risk Severity": "Major",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Data privacy risk; Legal risk; Reputational risk",
    "Id": 593
  },
  {
    "Summary": "Terms, licenses restrict AI model use",
    "Description": "Terms of service, licenses, or other rules may restrict the use of certain models.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk",
    "Id": 594
  },
  {
    "Summary": "AI responsibility hard without documentation",
    "Description": "Determining responsibility for an AI model is challenging without good documentation and governance.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Legal risk; Operational risk; Reputational risk",
    "Id": 595
  },
  {
    "Summary": "Legal uncertainty AI content ownership",
    "Description": "Legal uncertainty exists regarding ownership and IP rights of AI-generated content.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Financial risk; Legal risk; Strategic risk",
    "Id": 596
  },
  {
    "Summary": "Insufficient AI system documentation risks",
    "Description": "Insufficient documentation of the system using an AI model and the model's purpose within that system creates risks.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Operational risk; Reputational risk",
    "Id": 597
  },
  {
    "Summary": "Unrepresentative AI testing, unreliable evaluation",
    "Description": "Unrepresentative testing occurs when test inputs don't match expected deployment inputs, leading to unreliable evaluation.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Operational risk; Reputational risk; Technological risk",
    "Id": 598
  },
  {
    "Summary": "AI foundation model use changes risks",
    "Description": "A foundation model's intended use is crucial for defining its risks; as use changes, relevant risks may change.",
    "Risk Severity": "Moderate",
    "Likelihood": "Almost certain",
    "Risk Category": "Compliance risk; Operational risk; Strategic risk",
    "Id": 599
  },
  {
    "Summary": "AI data opacity hinders risk assessment",
    "Description": "Lack of data transparency from insufficient documentation of training/tuning dataset details hinders risk assessment.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Data privacy risk; Operational risk; Reputational risk",
    "Id": 600
  },
  {
    "Summary": "Incorrect AI risk metric, flawed management",
    "Description": "An incorrectly selected or incomplete metric for tracking a risk, or measuring the wrong risk for a given context, leads to flawed risk management.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Operational risk; Strategic risk",
    "Id": 601
  },
  {
    "Summary": "AI model opacity hinders understanding, trust",
    "Description": "Lack of model transparency from insufficient documentation of design, development, evaluation, and absence of insights into inner workings, hinders understanding and trust.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Operational risk; Reputational risk; Technological risk",
    "Id": 602
  },
  {
    "Summary": "Socio-technical AI risks need diverse input",
    "Description": "AI model risks being socio-technical require broad disciplinary input and diverse testing practices for effective management.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Operational risk; Strategic risk; Technological risk",
    "Id": 603
  },
  {
    "Summary": "AI overrepresents cultures, homogenization",
    "Description": "AI systems might overly represent certain cultures, leading to cultural homogenization and loss of diversity.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Reputational risk; Strategic risk",
    "Id": 604
  },
  {
    "Summary": "GenAI access leads to student plagiarism",
    "Description": "Easy access to high-quality generative models may lead to students intentionally or unintentionally plagiarizing existing work.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Fraud risk; Legal risk; Reputational risk",
    "Id": 605
  },
  {
    "Summary": "AI adoption leads to job losses",
    "Description": "Widespread adoption of foundation model-based AI systems might lead to job losses if workers are not reskilled for automated tasks.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Financial risk; Human resources risk; Strategic risk",
    "Id": 606
  },
  {
    "Summary": "Excluding community perspectives hinders trust",
    "Description": "Failing to include perspectives of communities affected by model outcomes hinders understanding of relevant context and trust-building.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Compliance risk; Reputational risk; Strategic risk",
    "Id": 607
  },
  {
    "Summary": "GenAI access bypasses student learning",
    "Description": "Easy access to high-quality generative models might result in students using AI to bypass the learning process.",
    "Risk Severity": "Moderate",
    "Likelihood": "Likely",
    "Risk Category": "Human resources risk; Reputational risk; Strategic risk",
    "Id": 608
  },
  {
    "Summary": "Large GenAI increases emissions, water use",
    "Description": "AI, particularly large generative models, might increase carbon emissions and water usage for training and operation, causing environmental harm.",
    "Risk Severity": "Major",
    "Likelihood": "Likely",
    "Risk Category": "Environmental risk; Financial risk; Reputational risk",
    "Id": 609
  },
  {
    "Summary": "Poor AI worker conditions ethical risk",
    "Description": "Inadequate working conditions, unfair compensation, or poor healthcare (including mental health) for workers training AI models (e.g., ghost workers) is an ethical risk.",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Compliance risk; Health and safety risk; Human resources risk; Legal risk; Reputational risk; Third-party/vendor risk",
    "Id": 610
  },
  {
    "Summary": "AI negatively affects individual autonomy",
    "Description": "AI might negatively affect individuals' ability to make choices and act independently in their best interests (loss of autonomy).",
    "Risk Severity": "Moderate",
    "Likelihood": "Possible",
    "Risk Category": "Health and safety risk; Reputational risk; Strategic risk",
    "Id": 611
  }
]